
## ユーザーへの通知:
**このリポジトリ内の資料は、TOPSTプラットフォームでの使用のみを目的としています。**

**明示的に許可されていない限り、第三者への再配布または共有は禁止されています。**
**これらの資料はTOPSTプラットフォームなしでは利用できず、TOPST環境外での使用は公式テクニカルサポートの対象外です。**

<br/><br/>

# 1. はじめに
---
Neural Processing Unit (NPU) はHWニューラルネットワークアクセラレータであり、TCC750xのNPUは、物体検出、画像分類、顔検出などのビジョンアプリケーション向けのニューラルネットワークの推論に最適化されています。
このドキュメントでは、NPUツールキットの使用方法について説明します。
<br/><br/><br/>

## 1.1 概要
---
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/tcnntoolkit.png" width="700"></p>
<p align="center"><strong>図 1.1 AI-G SW ツールキットの概要</strong></p><br/>

- Converter
  * ネットワークファイルを内部ネットワーク形式 (.bin) に変換します
  * ONNX(PyTorch)、TF-Lite、およびCFG(Darknet) をサポートします

<!-- -->

- Quantizer
  * 量子化されたネットワークを生成します: float から int8

<!-- -->

- Compiler
  * 量子化されたネットワークを入力として使用して、NPUマシンコードを生成します
<br/><br/><br/><br/>

# 2. AI-G NPUツールキットのダウンロード
---
この章では、プラットフォームでのモデルのコンパイルと実行に必要な必須ツールを含む、AI-G NPUツールキットのダウンロード情報を提供します。
- AI-G NPU Toolkit-v1.0.0 : [ダウンロードリンク](https://topst-downloads.s3.ap-northeast-2.amazonaws.com/Education/Motrex/tc-nn-toolkit.zip)
<br/><br/><br/>

## 2.1 AI-G NPUツールキットの構造
---
NPU開発
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/2.%20tc-nn-toolkit%20Directory.png" width="550"></p>
<p align="center"><strong>図 2.1 AI-G NPUツールキットディレクトリ</strong></p><br/>

**表 2.1 tc-nn-toolkit:**

|                ディレクトリ                        |        説明          |
|-------------------------------------------------|-----------------------------|
| tc-nn-toolkit | 作業ディレクトリ    |
| EnlightSDK               | NPU SDK ディレクトリ           |
| build_network                                   | ネットワークライブラリをビルドするためのディレクトリ |

AI-G NPUツールキットは、Ubuntu 20.04とPython 3.8を搭載した64ビットPCでテストされました。GPUは必要ありません。
<br/><br/><br/>

## 2.2 パッケージのインストール
---
Pythonパッケージをインストールする前に、Python3.8および関連ライブラリがインストールされていることを確認してください。AI-G NPUツールキットに必要なPythonパッケージは、ツールキットパッケージディレクトリ内のrequirements.txtファイルにリストされています。
```
$ sudo add-apt-repository ppa:deadsnakes/ppa -y
$ sudo apt install python3.8 python3.8-venv python3.8-dev python3.8-tk
$ python3.8 -m venv ./venv
$ source ./venv/bin/activate
$ pip install --upgrade pip 
Collecting pip
  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 20.9 MB/s
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.0.2
    Uninstalling pip-20.0.2:
      Successfully uninstalled pip-20.0.2
Successfully installed pip-23.3.2

$ pip install -r requirements.txt
Collecting certifi==2022.6.15 (from -r requirements.txt (line 1))
  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 14.2 MB/s eta 0:00:00
Collecting charset-normalizer==2.1.0 (from -r requirements.txt (line 2))
  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)
Collecting cycler==0.11.0 (from -r requirements.txt (line 3))
  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)
Collecting distro==1.7.0 (from -r requirements.txt (line 4))
  Downloading distro-1.7.0-py3-none-any.whl (20 kB)
Collecting flatbuffers==1.11 (from -r requirements.txt (line 5))
  Downloading flatbuffers-1.11-py2.py3-none-any.whl (15 kB)
Collecting fonttools==4.34.4 (from -r requirements.txt (line 6))
  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 18.6 MB/s eta 0:00:00
Collecting idna==3.3 (from -r requirements.txt (line 7))
Downloading idna-3.3-py3-none-any.whl (61 kB)
….

$ pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 --extra-index-url \https://download.pytorch.org/whl/cpu
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu
Collecting torch==1.12.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torch-1.12.0%2Bcpu-cp38-cp38-linux_x86_64.whl (189.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.0/189.0 MB 22.3 MB/s eta 0:00:00
Collecting torchvision==0.13.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.13.0%2Bcpu-cp38-cp38-linux_x86_64.whl (13.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 38.8 MB/s eta 0:00:00
$ pip install ./enlight_viewer/netron-3.5.4-py2.py3-none-any.whl
```
<br/><br/><br/>

## 2.3 サンプルネットワーク
---
- サンプルネットワークパス:
  input_networks/

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/3.%20toolkit_sample.png" width="550"></p>
<p align="center"><strong>図 2.2 サンプルネットワーク</strong></p><br/>

**表 2.2 サンプルネットワークの説明:**

|     マシン     |                        ファイル                           |
|-----------------|-------------------------------------------------------|
| Darknet         | yolov4.cfg, yolov4.weights                            |
| TensorFlow Lite | lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite |
| ONNX            | mb2-ssd-lite.onnx, mb2-ssd-lite.anchor                |
| ONNX_YOLO       | yolov3/5/6/7/8/x.bin, yolov3/5/6/7/8/x_extracted.onnx | 

<br/><br/><br/><br/>

# 3. ビルドガイド
---
この章では、AI-G NPUで実行するためのニューラルネットワークモデルを変換およびビルドするプロセスについて説明します。まず、標準モデルをNPUと互換性のある形式に変換するConverterツールの使用から始めます。
<br/><br/><br/>

## 3.1 Converter
---
Converterはニューラルネットワークファイルを読み取り、AI-G NPU SWツールキットで使用されるネットワークファイル形式である "bin" ファイルに変換します。
また、ネットワーク推論を実行し、後でネットワークの量子化に使用されるアクティベーションデータの統計を収集します。

<strong>1.  Darknet</strong>
```
$ python ./EnlightSDK/converter.py \
  ./input_networks/yolov4.cfg \
  --weight ./input_networks/yolov4.weights \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/yolov4.enlight \
  --enable-track \
  --mean 0 0 0 \
  --std 1 1 1 \
  --num-class 90
```

<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/yolov4.cfg</td>
<td>入力ネットワークファイル名</td>
</tr>
<tr class="even">
<td>--weight ./input_networks/yolov4.weights</td>
<td>Darknet重みファイル名</td>
</tr>
<tr class="odd">
<td>--type obj</td>
<td><p>後処理のタイプ:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="even">
<td>--dataset Custom</td>
<td>Custom: ユーザー定義データセット</td>
</tr>
<tr class="odd">
<td>--dataset-root ./my_dataset_path</td>
<td><p>アクティベーション統計の収集に使用される</p>
<p>入力データへのパス</p></td>
</tr>
<tr class="even">
<td>--output ./output_networks/yolov4.enlight</td>
<td>Converter出力ファイル名</td>
</tr>
<tr class="odd">
<td>--enable-track</td>
<td>アクティベーション統計の収集を有効にする</td>
</tr>
<tr class="even">
<td>--mean 0 0 0</td>
<td>入力正規化の平均値 (RGB)</td>
</tr>
<tr class="odd">
<td>--std 1 1 1</td>
<td>入力正規化の標準偏差値 (RGB)</td>
</tr>
<tr class="even">
<td>--num-class 90</td>
<td>オブジェクトクラス番号</td>
</tr>
</tbody>
</table>
<br/><br/>

<strong>2.  TensorFlow Lite</strong>
```
$ python ./EnlightSDK/converter.py \
  ./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --enable-track \
  --mean 0.5 0.5 0.5 \
  --std 0.5 0.5 0.5 \
  --num-class 90
```

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite</td>
<td>入力ネットワークファイル名</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>後処理のタイプ:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: ユーザー定義データセット</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>アクティベーション統計の収集に使用される</p>
<p>入力データへのパス</p></td>
</tr>
<tr class="odd">
<td>--output
./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</td>
<td>Converter出力ファイル名</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>アクティベーション統計の収集を有効にする</td>
</tr>
<tr class="odd">
<td>--mean 0.5 0.5 0.5</td>
<td>入力正規化の平均値 (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.5 0.5 0.5</td>
<td>入力正規化の標準偏差値 (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 90</td>
<td>オブジェクトクラス番号</td>
</tr>
</tbody>
</table>
<br/><br/>

<strong>3.  ONNX</strong>

ONNXモデルにサポートされていない演算子 (例: NonMaxSuppression) が含まれている場合は、この方法が必要です。

以下のプロセスには、tc-nn-toolkitコマンドラインインターフェイスを使用した3段階の変換が含まれます。


まず、ONNXモデルを.enlight形式に変換します。

```
$ python ./EnlightSDK/converter.py \
  ./input_networks/mb2-ssd-lite.onnx \
  --type obj \
  --add-detection-post-process ./input_networks/mb2-ssd-lite.anchor \
  --logistic softmax \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/mb2-ssd-lite.enlight \
  --enable-track \
  --mean 0.498 0.498 0.498 \
  --std 0.502 0.502 0.502 \
  --num-class 20 \
  --variance 0.1 0.2
```

次に、--force-outputを使用して中間層を出力としてモデルを抽出し、サポートされていない演算子を削除します。

```
python ./EnlightSDK/converter.py \
            ./input_networks/mb2-ssd-lite.onnx \
            --type unknown \
            --output ./output_networks/mb2-ssd-lite_wo_box_decode_score.enlight \
            --dataset-root my_dataset_path \
            --force-output Concat_0___Concat Concat_1___Concat_1
```

最後に、アンカーボックス、ロジスティックアクティベーション、クラス設定などの後処理オプションを抽出されたモデルに適用して、最終的な.enlightモデルを生成します。

```
$ python ./EnlightSDK/converter.py \
  ./output_networks/mb2-ssd-lite_wo_box_decode_score.enlight \
  --type obj \
  --add-detection-post-process ./input_networks/mb2-ssd-lite.anchor \
  --logistic softmax \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/mb2-ssd-lite.enlight \
  --enable-track \
  --mean 0.498 0.498 0.498 \
  --std 0.502 0.502 0.502 \
  --num-class 20 \
  --variance 0.1 0.2
```
<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/mb2-ssd-lite_wo_box_decode_score.enlight</td>
<td>入力ネットワークファイル名</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>後処理のタイプ:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--add-detection-post-process
./input_networks/mb2-ssd-lite.anchor</td>
<td>デフォルトボックスファイル名</td>
</tr>
<tr class="even">
<td>--logistic softmax</td>
<td><p>後処理のロジスティック関数:</p>
<p>softmax, sigmoid, none</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: ユーザー定義データセット</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>アクティベーション統計の収集に使用される</p>
<p>入力データへのパス</p></td>
</tr>
<tr class="odd">
<td>--output ./output_networks/mb2-ssd-lite.enlight</td>
<td>Converter出力ファイル名</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>アクティベーション統計の収集を有効にする</td>
</tr>
<tr class="odd">
<td>--mean 0.498 0.498 0.498</td>
<td>入力正規化の平均値 (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.502 0.502 0.502</td>
<td>入力正規化の標準偏差値 (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 20</td>
<td>オブジェクトクラス番号</td>
</tr>
<tr class="even">
<td>--variance 0.1 0.2</td>
<td>SSD検出レイヤーのxおよびyの分散</td>
</tr>
</tbody>
</table>
<br/><br/><br/>

## 3.2 Quantizer
---
Quantizerは、浮動小数点演算に基づく元のネットワークモデルを整数ベース (量子化) のネットワークに変換します。

**1. Darknet**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/yolov4.enlight \
  --output ./output_networks/yolov4_quantized.enlight
```

**2. TensorFlow Lite**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight
```

**3. ONNX**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/mb2-ssd-lite.enlight \
  --output ./output_networks/mb2-ssd-lite_quantized.enlight
```
<br/><br/><br/>

## 3.3 Compiler
---
Compilerは、AI-G NPU HWでニューラルネットワークを処理するために、量子化されたニューラルネットワークをNPUコードとネットワークパラメータにコンパイルします。

**1.  Darknet**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/yolov4_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

**2.  TensorFlow Lite**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

**3.  ONNX**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/mb2-ssd-lite_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<tbody>
<tr class="odd">
<td>--th-iou 0.5</td>
<td><p>IOUしきい値。</p>
<p>検出ニューラルネットワークでのみ有効です。</p></td>
</tr>
<tr class="even">
<td>--th-conf 0.5</td>
<td><p>信頼度しきい値。</p>
<p>検出ニューラルネットワークでのみ有効です。</p></td>
</tr>
</tbody>
</table>

生成されたファイルは “output_code/\<network_name\>” ディレクトリに保存されます。

Compiler出力ファイル:
- npu_cmd.bin: Telechips NPU推論コード用の推論コード
- quantized_network.bin: ネットワークの重みとバイアスのパラメータ
- network.h: 後処理用のバッファサイズとネットワークパラメータ
- post_process.c: 後処理コード
- その他のファイル: デバッグ情報 (内部のみ)

ニューラルネットワークアプリケーションでは、“npu_cmd.bin” と “quantized_network.bin” は、Telechips NPU APIとLinuxデバイスドライバを使用してDMAバッファにロードされます。“network.h” と “post_process.c” は、ネットワークオブジェクトビルドに示すように、ネットワークオブジェクトファイル (.so) としてビルドされます。
<br/><br/><br/>

## 3.4 ネットワークオブジェクトビルド
---
この章では、コンパイルされたニューラルネットワークファイルを使用してネットワークオブジェクトファイルをビルドする方法について説明します。“build_network” ディレクトリには、AI-Gで使用されるネットワークモデルの共有ライブラリファイル (.so) をビルドするためのコンパイル環境が含まれています。
<br/><br/>

### 3.4.1 ツールチェーンのインストール (gcc-arm-9.2)
```
$ wget https://developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ tar -xvf gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ echo "export PATH=~/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin:\$PATH" >> ~/.bashrc
$ source ~/.bashrc
$ aarch64-none-linux-gnu-gcc --version
aarch64-none-linux-gnu-gcc (GNU Toolchain for the A-profile Architecture 9.2-2019.12 (arm-9.10)) 9.2.1 20191025
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```
<br/><br/>

### 3.4.2 オブジェクトのビルド
Compiler出力ディレクトリを “build_network” ディレクトリにコピーします。
makeコマンドを実行する前に、build_networkフォルダ内の既存のpost_process.cおよびnetwork.hファイルを削除する必要があります。
次に、コンパイルされた <network_name>_quantized ディレクトリからbuild_networkフォルダへのnetwork.hおよびpost_process.cファイルのシンボリックリンクを作成します。

```
$ cd build_network
$ cp ../output_code/<network_name>_quantized/ ./ -ar
$ rm -rf network.h post_process.c
$ ln -s ./<network_name>_quantized/network.h
$ ln -s ./<network_name>_quantized/post_process.c
$ make
$ cp ./net.so ./<network_name>_quantized/
$ ls -l <network_name>_quantized/
compile_debug.log
net.so
network.h
network_buf.txt
network_group_info.csv
      ㆍ
      ㆍ
      ㆍ
```
<br/><br/><br/><br/>

# 4. ネットワークのダウンロード
---
#### ステップ 1. カメラとDSIディスプレイを接続し、AI-Gを起動します。
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/7.%20connect.png"</p>
<p align="center"><strong>図 4.1 カメラとディスプレイの接続</strong></p><br/>

#### ステップ 2. “scp” コマンドを使用してネットワークフォルダを転送します。

    ```
    $ scp -r yolov4_quantized/ root@192.168.0.100:/home/root
    The authenticity of host '192.168.0.100 (192.168.0.100)' can't be established.
    ED25519 key fingerprint is SHA256:UCT26gsSk4PfzL/lZ4c3o4RM/MSl16mQ49XSn7FRx/s.
    This key is not known by any other names
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    Warning: Permanently added '192.168.0.100' (ED25519) to the list of known hosts.	
    root@192.168.0.100's password: //<- enter root
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/4.%20scp%20Command.png"</p>
<p align="center"><strong>図 4.2 scp コマンド</strong></p><br/>

#### ステップ 3. ホストPCから転送されたフォルダがAI-Gターミナルに表示されることを確認します。

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/5.%20Check%20Download.png" width="550"></p>

<p align="center"><strong>図 4.3 ダウンロードの確認</strong></p><br/>

#### ステップ 4. AI-Gターミナルで以下のコマンドを入力して、サンプルアプリケーションを実行します。

    ```
    $ tcnnapp -p /dev/video0 -W 800 -H 480 -w 1920 -h 1080 -n yolov4_quantized/
    ===================================================================================================
    ==   _________  _______   ___       _______   ________  ___  ___  ___  ________  ________        ==
    ==  |\___   ___\\  ___ \ |\  \     |\  ___ \ |\   ____\|\  \|\  \|\  \|\   __  \|\   ____\       ==
    ==  \|___ \  \_\ \   __/|\ \  \    \ \   __/|\ \  \___|\ \  \\\  \ \  \ \  \|\  \ \  \___|_      ==
    ==       \ \  \ \ \  \_|/_\ \  \    \ \  \_|/_\ \  \    \ \   __  \ \  \ \   ____\ \_____  \     ==
    ==        \ \  \ \ \  \_|\ \ \  \____\ \  \_|\ \ \  \____\ \  \ \  \ \  \ \  \___|\|____|\  \    ==
    ==         \ \__\ \ \_______\ \_______\ \_______\ \_______\ \__\ \__\ \__\ \__\     ____\_\  \   ==
    ==          \|__|  \|_______|\|_______|\|_______|\|_______|\|__|\|__|\|__|\|__|    |\_________\  ==
    ==                                                                                 \|_________|  ==
    ===================================================================================================

    ----------------Parameter Info----------------

    [Network1]Network Path         : yolov4_quantized/
    [Network2]Network Path         : /usr/share/mobilenetv2_10_quantized/

    [Common]Input Mode             : camera
    [Common]Output Mode            : display
    [Common]Input Size             : 1920 x 1080
    [Common]Output Size            : 800 x 480
    [Common]Input Path             : /dev/video0
    [Common]Output Path            : /dev/overlay
    [Common]Output Position X      : 0
    [Common]Output Position Y      : 0

    [Debug]Debug Mode              : off
    [Debug]NPU Debug Mode          : off
    [Debug]NPU Running Mode        : AsyncMode
    ----------------Parameter End----------------
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/6.%20Sample%20Applictation.png" width="500"></p>
<p align="center"><strong>図 4.4 サンプルアプリケーション実行画面</strong></p><br/><br/><br/><br/>

# 5. NPU互換演算子
---
この章では、NPUでサポートされている演算子を紹介します。以下のリストには、プラットフォームと互換性のある主要な演算が含まれています。
<br/><br/><br/>

## 5.1 AI-G NPUサポートレイヤータイプ
---
AI-G NPUは、以下のレイヤータイプをサポートしています。

**表 5.1 サポートされているレイヤータイプと仕様:**

|        レイヤータイプ       |                        仕様                        |
|-------------------------|-------------------------------------------------------------|
| Convolution             | カーネルサイズ: 1x1 から 7x7<br/>ストライド: 1 から 4<br/>パディング: 0 から 3 <br/>ダイレーション: 1 から 4 |
| ConvTranspose           | カーネルサイズ: 2x2 から 3x3<br/>ストライド: 2<br/>パディング: 0 から 1  |
| Depth-wise Convolution  | カーネルサイズ: 1x1 から 5x5<br/>ストライド: 1 から 2<br/>パディング: 0 から 2|
| Activation Function     | Bypass<br/>ReLU<br/>ReLU6<br/>Leaky Relu <br/> Sigmoid <br/> Tanh <br/> Mish <br/> Swish <br/> Clip <br/> HardSwish <br/> HardSigmoid| 
| Max Pool 2D             | カーネルサイズ: 2x2 から 13x13<br/>ストライド: 1 から 2<br/>パディング: 0 から 6| 
| Average Pool 2D         | カーネルサイズ: 2x2 から 19x19<br/>ストライド: 1 から 2<br/>パディング: 0 から 6| 
| Global Average Pool     | サポートあり                                                   | 
| Element-wise Operation  | 加算: スカラー + テンソル, ベクトル + テンソル, テンソル + テンソル <br/>  乗算: スカラー * テンソル, ベクトル * テンソル, テンソル * テンソル                    |  
| Batch Normalization     | サポートあり                                                   |  
| Concatenation           | チャンネル方向のみ                                           | 
| Slice                   | サポートあり                                                   |
| Split                   | サポートあり                                                   |

<br/><br/><br/>

## 5.2 PyTorch演算子 (ONNX)
---
演算子がAI-G NPUでサポートされていない場合でも、演算がニューラルネットワークの最終層にある場合は、ホストCPUで実行できます。

**注**: Single Shot MultiBox Detector (SSD) ネットワークのSoftmaxレイヤーは、後処理コードでCPUによって処理されます。


|         演算子        |               専用HW             |    ベクトルプロセッサ    |
|-------------------------|----------------------------------------|------------------------|
| Conv                    | カーネル: 1x1 から 7x7<br/>パディング: 0 から 3<br/>ストライド: 1 から 4<br/>ダイレーション: 1 から 4  | 
| Conv Transpose          |  カーネル: 2x2 から 3x3<br/>パディング: 0 から 1<br/>ストライド: 2<br/>出力パディング: 0 から 1  |                                              |  
| Dw-conv                 | カーネル: 1x1 から 5x5<br/>パディング: 0 から 2<br/>ストライド: 1 から 2<br/>ダイレーション: 1  |
| Activation              | Relu<br/> LeakyRelu<br/> Sigmoid<br/> Tanh<br/> Mish<br/> Clip (min = 0, のみ)<br/> HardSwish<br/> HardSigmoid |
| MaxPool                 | カーネル: 2x2 のみ <br/> ストライド: 2 のみ<br/> | カーネル: 2x2 から 13x13<br/>パディング: 0 から 6<br/>ストライド: 1 から 2   |
| AveragePool             | カーネル: 2x2 から 19x19<br/>パディング: 0 から 6<br/>ストライド: 1 から 2 |
| GlobalAveragePool       | O                                                           |
| Add                     | スカラー + テンソル <br/> テンソル + テンソル <br/> | ベクトル + テンソル |
| Sub                     | O (Add演算子に変換)                                 |
| Mul                     | スカラー * テンソル <br/> テンソル * テンソル<br/> | テンソル * テンソル |                                                   |
| Div                     | O (定数除数のみ)                                   |
| Resize                  | 最近傍: 2x, 4x, 8x (=2^n) <br/>バイリニア: 2x, 4x, (align_core = false, のみ) |
| Upsample                | ストライド: 2 のみ                                              |
| Batchnorm               | O                                                           |
| Pad                     | パディング: 0 から 3 (ゼロパディングのみ)                         |
| Concat                  | 方向: チャンネル方向のみ                                |
| Slice                   | O                                                           |
| Split                   | O                                                           |
| Squeeze                 | O                                                           |
| Unsqueeze               | O                                                           |
| MatMul                  | 1つのテンソルは定数である必要があります。                              |
| Gemm                    | O                                                           |
| Flatten                 | O                                                           |
| ReduceMean              | 軸: [h,w] のみ                                            |
| ReduceSum               | 軸: [h,w] のみ                                            |
| Constant                | O                                                           |
| ConstantOfShape         | O                                                           |

<br/><br/><br/>

## 5.3 Darknet演算子
---
演算子がAI-G NPUでサポートされていない場合でも、演算がニューラルネットワークの最終層にある場合は、ホストCPUで実行できます。

|         演算子        |                専用HW              |  ベクトルプロセッサ  |ホストCPU |
|-------------------------|------------------------------------------|--------------------|--|
| Conv2d                  | カーネル: 1x1 から 7x7<br/>パディング: 0 から 3<br/>ストライド: 1 から 4<br/>ダイレーション: 1 から 4 |
| Deconvolution           | カーネル: 2x2 から 3x3<br/>パディング: 0 から 1<br/>ストライド: 2        |
| Dw-conv                 | カーネル: 1x1 から 5x5<br/>パディング: 0 から 2<br/>ストライド: 1 から 2<br/>ダイレーション: 1|
| Activation              | Relu<br/>Relu6<br/>Leaky<br/>Logistic<br/>Tanh<br/>Mish<br/>Swish|
| MaxPool                 | カーネル: 2x2 のみ<br/>ストライド: 2 のみ<br/> | カーネル: 2x2 から 13x13<br/>パディング: 0 から 6 <br/>ストライド: 1 から 2  |
| Local_avgpool           | カーネル: 2x2 から 19x19<br/>パディング: 0 から 6<br/>ストライド: 1 から 2 |
| Avgpool                 | O                                                           |
| Upsample                | ストライド: 2 のみ                                              |
| Batchnorm               | O                                                           |
| Route                   | 方向: チャンネル方向のみ (32アライメントされたチャンネルサイズのみ) |
| Connected               | O                                                           |
| Reorg3d                 | O                                                           |
| Shortcut                | O                                                           |
| Sam                     | O                                                           |
| Region                  | | | O                                         |
| Yolo                    | | | O                                         |

<br/><br/><br/>

## 5.4 Tensor Flow Lite演算子
---
演算子がAI-G NPUでサポートされていない場合でも、演算がニューラルネットワークの最終層にある場合は、ホストCPUで実行できます。

|         演算子        |         専用HW        |        ベクトルプロセッサ       |
|-------------------------|-----------------------------|-------------------------------|
| CONV_2D                 | カーネル: 1x1 から 7x7<br/>パディング: 0 から 3<br/>ストライド: 1 から 4<br/>ダイレーション: 1 から 4 |
| TRANSPOSE_CONV          | カーネル: 2x2 から 3x3<br/>パディング: Same, Valid<br/>ストライド: 2   |
| DEPTHWISE_CONV_2D       | カーネル: 1x1 から 5x5<br/>パディング: 0 から 2<br/>ストライド: 1 から 2<br/>ダイレーション: 1|
| Activation              | RELU<br/>RELU6<br/>LEAKY_RELU<br/>LOGISTIC<br/>HARD_SWISH   |
| MAX_POOL_2D             | カーネル: 2x2 のみ<br/>ストライド: 2 のみ <br/> | カーネル: 2x2 から 13x13<br/> パディング: 0 から 6<br/>ストライド: 1 から 2|
| AVERAGE_POOL_2D         | カーネル: 2x2 から 19x19<br/> パディング: 0 から 6<br/>ストライド: 1 から 2|  
| ADD                     | スカラー + テンソル<br/>テンソル + テンソル<br/> | ベクトル + テンソル     | 
| SUB                     | O (ADDレイヤーに変換)                                  | 
| MUL                     | スカラー * テンソル<br/>テンソル * テンソル<br/> | テンソル * テンソル     | 
| DIV                     | O(定数除数のみ)                                 |
| RESIZE                  | 最近傍: 2x, 4x, 8x (=2^n)<br/>バイリニア: 2x, 4x|
| PAD                     | パディング: 0 から 3 (ゼロパディングのみ)                         |
| CONCATENATION           | 方向: チャンネル方向のみ                                | 
| FULLY_CONNECTED         | O                                                           |
| MEAN                    | 軸: [h,w] のみ                                            |   


