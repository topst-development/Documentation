# Build Guide

## Converter

The Converter reads neural network file and converts it into ”enlight”
file, which is network file format used in TOPST AI NPU SW Toolkit.

And it also performs network inferences and collects statistics of
activation data, which are used later for network quantization.

1.  Darknet

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/converter.py</p>
<p>./input_networks/yolov4.cfg</p>
<p>--weight ./input_networks/yolov4.weights</p>
<p>--type obj</p>
<p>--dataset Custom</p>
<p>--dataset-root my_dataset_path</p>
<p>--output ./output_networks/yolov4.enlight</p>
<p>--enable-track</p>
<p>--mean 0 0 0</p>
<p>--std 1 1 1</p>
<p>--num-class 80</p></td>
</tr>
</tbody>
</table>

<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/yolov4.cfg</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--weight ./input_networks/yolov4.weights</td>
<td>Darknet weights file name</td>
</tr>
<tr class="odd">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="even">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="odd">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="even">
<td>--output ./output_networks/yolov4.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="odd">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="even">
<td>--mean 0 0 0</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--std 1 1 1</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>

2.  TensorFlow Lite

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/converter.py</p>
<p>./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite</p>
<p>--type obj</p>
<p>--dataset Custom</p>
<p>--dataset-root my_dataset_path</p>
<p>--output./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</p>
<p>--enable-track</p>
<p>--mean 0.5 0.5 0.5</p>
<p>--std 0.5 0.5 0.5</p>
<p>--num-class 90</p></td>
</tr>
</tbody>
</table>

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output
./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.5 0.5 0.5</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.5 0.5 0.5</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>

3.  ONNX

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/converter.py</p>
<p>./input_networks/mb2-ssd-lite.onnx</p>
<p>--type obj</p>
<p>--add-detection-post-process ./input_networks/mb2-ssd-lite.anchor</p>
<p>--logistic softmax</p>
<p>--dataset Custom</p>
<p>--dataset-root my_dataset_path</p>
<p>--output ./output_networks/mb2-ssd-lite.enlight</p>
<p>--enable-track</p>
<p>--mean 0.498 0.498 0.498</p>
<p>--std 0.502 0.502 0.502</p>
<p>--num-class 20</p>
<p>--variance 0.1 0.2</p></td>
</tr>
</tbody>
</table>

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/mb2-ssd-lite.onnx</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--add-detection-post-process
./input_networks/mb2-ssd-lite.anchor</td>
<td>Default box file name</td>
</tr>
<tr class="even">
<td>--logistic softmax</td>
<td><p>Logistic function in post process:</p>
<p>softmax, sigmoid, none</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output ./output_networks/mb2-ssd-lite.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.498 0.498 0.498</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.502 0.502 0.502</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 20</td>
<td>object class number</td>
</tr>
<tr class="even">
<td>--variance 0.1 0.2</td>
<td>Variance of x and y for SSD detection layer</td>
</tr>
</tbody>
</table>

## Quantizer

The Quantizer converts the original network model based on floating
point operations into integer-based (quantized) network.

1.  Darknet

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/quantizer.py</p>
<p>./output_networks/yolov4.enlight</p>
<p>--output ./output_networks/yolov4_quantized.enlight</p></td>
</tr>
</tbody>
</table>

2.  TensorFlow Lite

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/quantizer.py</p>
<p>./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</p>
<p>--output
./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight</p></td>
</tr>
</tbody>
</table>

3.  ONNX

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/quantizer.py</p>
<p>./output_networks/mb2-ssd-lite.enlight</p>
<p>--output ./output_networks/mb2-ssd-lite_quantized.enlight</p></td>
</tr>
</tbody>
</table>

## Compiler

The Compiler compiles a quantized neural network into NPU codes and
network parameters for processing neural networks on TOPST NPU HW.

1.  Darknet

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/compiler.py</p>
<p>./output_networks/yolov4_quantized.enlight</p>
<p>--th-iou 0.5</p>
<p>--th-conf 0.5</p></td>
</tr>
</tbody>
</table>

2.  TensorFlow Lite

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/compiler.py</p>
<p>./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight</p>
<p>--th-iou 0.5</p>
<p>--th-conf 0.5</p></td>
</tr>
</tbody>
</table>

3.  ONNX

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p>(venv)$ python ./EnlightSDK/compiler.py</p>
<p>./output_networks/mb2-ssd-lite_quantized.enlight</p>
<p>--th-iou 0.5</p>
<p>--th-conf 0.5</p></td>
</tr>
</tbody>
</table>

<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<tbody>
<tr class="odd">
<td>--th-iou 0.5</td>
<td><p>IOU threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
<tr class="even">
<td>--th-conf 0.5</td>
<td><p>Confidence threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
</tbody>
</table>

The generated files are stored in “output_code/\<network_name\>”
directory.

Compiler Output File:

- npu_cmd.bin: ENLIGHT NPU inference code

- quantized_network.bin: Network weight and bias parameters

- network.h: Buffers size and network parameters for post-process

- post_process.c: Post-process code

- Other files: Debugging information for only OPENEDGES’ developers

In neural network application, “npu_cmd.bin” and “quantized_network.bin”
are loaded into DMA buffers using ENLIGHT NPU APIs and Linux device
driver. “network.h” and “post_process.c” are built as network object
file (.so) as shown in Network Object Build.
