# 1 NPU INFERENCE FLOW

<p align="center"><img src="https://github.com/topst-development/Documentation/blob/main/TOPST-AI/Software/media/1. NPU Inference Flow.image1.png?raw=true"
style="width:2.36617in;height:6.93698in"</p>

<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Step Description</strong></td>
<td><strong>Step Description</strong></td>
</tr>
<tr class="even">
<td>Open</td>
<td>Call npu_open to generate NPU Device Handle.</td>
</tr>
<tr class="odd">
<td>Update MLX Firmware</td>
<td>Load MLX Kernel firmware through npu_init_mlx and enable MLX.</td>
</tr>
<tr class="even">
<td>Create Network</td>
<td>Create a network handle using npu_load_network.</td>
</tr>
<tr class="odd">
<td>Create Input Buffer</td>
<td>Create an Input buffer through buffer_alloc.</td>
</tr>
<tr class="even">
<td>Create Output Buffer</td>
<td>Create an Onput buffer through buffer_alloc.</td>
</tr>
<tr class="odd">
<td>Read Input Image</td>
<td><p>The input buffer address is obtained through buffer_get_addr.</p>
<p>Load the image into the input buffer.</p>
<p>The original image data may require manipulation such as resize
according to the</p>
<p>neural network input image size and color format.</p></td>
</tr>
<tr class="even">
<td>Run Network Inference</td>
<td><p>Run network_issue_run to call the RUN_INFERENCE command to
HW.</p>
<p>Run network_wait_done to wait for the RUN_INFERENCE command to
end.</p></td>
</tr>
<tr class="odd">
<td>Read Output Tensor</td>
<td>Get output buffer address from buffer_get_addr</td>
</tr>
<tr class="even">
<td>Run Post Process</td>
<td>Post processing (if necessary)</td>
</tr>
</tbody>
</table>
