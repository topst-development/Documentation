# 1. Introduction
This document provides usage guidance for the sample and test applications included in the AI-G SDK. These applications are designed to help users understand and evaluate AI inference capabilities using the onboard NPU of the AI-G.  
This document includes information on the following:
- Sample Applications
  - tcnnapp
  - tcnncameraapp
  - tcnputestapp
<br/><br/><br/><br/>
 
 
# 2. Sample Applications
This section describes the three sample/test applications provided in the AI-G SDK. Each application is designed to help users evaluate and utilize the AI capabilities of the AI-G. The applications cover various scenarios such as NPU inference testing and real-time camera input.
<br/><br/><br/>
 
## 2.1 tcnnapp
tcnnapp is a sample AI application that demonstrates how to perform image-based inference using the NPU on the AI-G platform. It supports various input/output modes and showcases object detection using pre-trained models such as YOLOv5s and MobileNetv2.
<br/><br/>
 
### Use case 1. Basic File to File Inference
This case demonstrates how to run inference on a local image file and save the result to an output file, which is useful for basic functional testing and model verification without using camera or display hardware.  
For inference, enter the following command:
```
$ tcnnapp -i file -p test.png -o file -P result.png
```
 
This command runs tcnnapp using an input image file (test.png) and saves the inference result to an output file (result.png). If you do not specify a model, the application automatically uses the default quantized models YOLOv5s and MobileNetv2.  
The results are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Available%20Applications/Figure%201.%20Inference%20Result.png"></p>
<p align="center"><strong>Figure 2.1 Inference Result </strong></p>
<br/><br/>
 
### Use case 2. Real-time Inference with Camera and Display
This case demonstrates how to use tcnnapp with a connected MIPI CSI camera and MIPI DSI display to perform real-time AI inference and visualize the results. The application streams live video input from the camera, performs real-time inference using the default quantized models (YOLOv5s and MobileNetv2) on the NPU, and displays the results on the screen.  
Device connections are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Available%20Applications/Figure%202.%20Device%20Connection.png" width="600"></p>
<p align="center"><strong>Figure 2.2 Camera and Display Connections on AI-G </strong></p><br/>
 
After connecting the device, enter the following command:
```
$ tcnnapp -i camera -o display
```
This command starts real-time inference using the CSI camera as input. The captured video frames are fed into the NPU for object detection and classification tasks.
The inference results, such as bounding boxes, class labels, and confidence scores, are rendered directly onto the output frames.
These annotated frames are displayed in real time on the 5-inch DSI display, providing you with immediate visual feedback.
 
In addition to visual output, the system logs key performance indicators such as CPU usage, NPU utilization, and memory status, which can be monitored through serial console or remote interface.
 
The results are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/6.%20Sample%20Applictation.png" width="500"></p>
<p align="center"><strong>Figure 2.3 Inference Result </strong></p><br/>
 
<br/><br/>
 
### Use case 3. Real-time Inference with Camera and RTPM
This case demonstrates how to use tcnnapp with a connected MIPI CSI camera as input and Real-time Performance Monitor (RTPM) as the output method. This application streams live video input from the camera, performs real-time inference using the default quantized models (YOLOv5s and MobileNetv2) on the NPU, and sends the results to the PC through RTPM for visualization and performance monitoring.  
The RTPM toolkit  is a GUI-based tool developed in Python that monitors output from the AI-G in a PC environment. It allows you to observe inference results as well as system performance merics such as CPU, NPU, and memory usage.  
 
**Note**: For more details about RTPM, refer to RTPM.md in the Software section.
 
Device connections are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Available%20Applications/Figure%204.%20Camera%20and%20RTPM%20Communication%20via%20AI-G.png"width="900"></p>
<p align="center"><strong>Figure 2.4 Camera and RTPM Communication Through AI-G </strong></p><br/>
 
After connecting the camera, enter the following command:
```
$ tcnnapp -i camera -o rtpm
```
 
This command starts real-time inference using the CSI camera as input. The inference results, such as detected objects and classification labels, are transmitted to the host PC and visualized through the RTPM interface.  
In addition to inference output, RTPM also displays system performance metrics including CPU usage, NPU load, and memory usage in real time.  
The results are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Available%20Applications/Figure%205.%20Inference%20Result.png"width="1600"></p>
<p align="center"><strong>Figure 2.5 Inference Result </strong></p>
<br/><br/>
 
### Use case 4. Inference with RTPM Input and Output
This case demonstrates how to run inference using an input image streamed form the host PC through RTPM, and to send the inference results back to the PC for display through the same RTPM interface. The input data can be an image or a video.  
Refer to the RTPM.md document in the Software section to run RTPM. Then, enter the following command:
```
$ tcnnapp -i rtpm -o rtpm
```
This command runs tcnnapp using RTPM as both the input and output. The data is sent from the host PC through RTPM, processed by the NPU on the AI-G board, and the inference result is sent back to the PC and displayed through the RTPM interface.  
The results are as follows:
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Available%20Applications/Figure%206.%20Inference%20Result.png"></p>
<p align="center"><strong>Figure 2.6 Inference Result </strong></p>
<br/><br/><br/>
 
## 2.2 tcnncameraapp
tcnncameraapp is a sample application that allows you to test and preview camera modules supported on the AI-G platform. It is built on the Video4Linux2 (V4L2) framework and enables real-time camera preview through a connected MIPI CSI interface and DSI display.  
This application is primarily used for verifying camera functionality, testing display output, and debugging image signal quality. It also supports simultaneous monitoring of multiple cameras depending on the camera module type.  
The options provided by tcnncameraapp are shown in the following table.
 
| Name              | Option  | Range     | Default     | Example        | Description                           |
| ------------------| --------| ----------| ------------| ---------------| --------------------------------------|
| Usage info        | -?      |     -     |             | -?             | Show the usage information            |
| Input Path        | -p      |     -     | /dev/video0 | -p /dev/video0 | Specify the input path                |
| Output Path       | -P      |     -     | /dev/overlay| -p /dev/overlay| Specify the output path               |
| Input Width       | -w      | Up to 2560| 1920        | -w 2560        | Specify the width of the input        |
| Input Height      | -h      | Up to 1440| 1080        | -h 1080        | Specify the height of the input       |
| Output Width      | -W      | Up to 1920| 1920        | -w 1920        | Specify the width of the output       |
| Output Height     | -H      | Up to  720| 720         | -H 720         | Specify the height of the output      |
| Output Position X | -X      | Up to 1920| 0           | -X 960         | Specify the X coordinate of the output|
| Output Position Y | -Y      | Up to  720| 0           | -Y 360         | Specify the Y coordinate of the output|
<p align="center"><strong>Table 2.1 Camera Test Application Options</strong></p><br/>
 
**Note**: The sum of the output width and its X position must not exceed 1920, and the sum of the output height and its Y position must not exceed 720.
 
The result of running the application is as follows:
```
root@ai-g-topst:~# tcnncameraapp
------------------ Parameter Info ------------------
[Common]Input Path             : /dev/video0
[Common]Output Path            : /dev/overlay
[Common]Input Width            : 1280
[Common]Input Height           : 720
[Common]Output Width           : 800
[Common]Output Height          : 480
[Common]Output Position X      : 0
[Common]Output Position Y      : 0
[Debug] Debug Mode             : off
---------------------- Usage ----------------------
 
[INFO] [SCALER_API] scaler /dev/scaler1 open() success:, fd:5
[INFO] [CAMERA_API] [CameraSetConfig] mmap idx : 0, len : 3686400, off :        0, reserved : 86b00000
[INFO] [CAMERA_API] [CameraSetConfig] mmap idx : 1, len : 3686400, off :   384000, reserved : 86f00000
[INFO] [CAMERA_API] [CameraSetConfig] mmap idx : 2, len : 3686400, off :   708000, reserved : 87300000
```
 
After launching tcnncameraapp, the system allocates multiple frame buffers using memory mapping (mmap) for real-time camera previw. Each buffer is indexed and mapped to a specific physical address space for efficient video streaming and rendering.
<br/><br/><br/>
 
## 2.3 tcnputestapp
tcnputestapp is a sample application that verifies the basic functionality of the Neural Processing Unit (NPU) on the AI-G platform. It utilizes the tcndnpulib library to directly test NPU execution with pre-compiled neural network models. This application is primarily intended for internal validation, debugging, and performance testing of the NPU without camera or display involvement.  
The options provided by tcnputestapp are shown in the following table.
 
| Name           | Option | Range     | Default     | Example                  | Description                           |
| ---------------| -------| ----------| ------------| -------------------------| --------------------------------------|
| Usage info     | -?     |     -     |      -      | -?                       | Show the usage information            |
| NPU Number     | -d     |    0, 1   |      0      | -d 0 / -d 1 / -d 0 1     | Specify the NPU index of network      |
| Network Path   | -n     |     -     | /usr/share/yolov5s_quantized| -n /usr/share/mobilenetv2_10_quantized/   | Specify the neural network to test. If two NPUs are selected, they have the same neural network path|
| Number of tests| -t     | -         | 1           | -t 100                   | Specify the number of neural network tests|
<p align="center"><strong>Table 2.2 NPU Test Application Options</strong></p><br/>
 
The result of running the application is as follow:
```
root@ai-g-topst:~# tcnputestapp
npu 0 open success
========================
   NPU Device Info
========================
NPU product id: 0xED9E
NPU major ver : 0x2
NPU minor ver : 0x0
NPU ecc       : 0x1
NPU core num  : 0x2
========================
 
NPU0 Network: /usr/share/yolov5s_quantized
NPU0 Performance Statistics for 1 runs
NPU0 Elapsed Time  (ms) - Min: 14.20     Max: 14.20     Avg: 14.20
NPU0 DMA Count  (@pclk) - Min: 992881    Max: 992881    Avg: 992881.00
NPU0 Comp Count (@pclk) - Min: 2218197   Max: 2218197   Avg: 2218197.00
NPU0 Utilization    (%) - Min: 29.30     Max: 29.30     Avg: 29.30
 
npu 0 close
========================
npu test: Success!
```
 
After executing tcnputestapp, the NPU is initialized and performs inference using the default YOLOv5s quantized model. Performance metrics such as execution time, DMA/computation cycle counts, and utilization rate are reported, providing a quick verification of NPU functionality.