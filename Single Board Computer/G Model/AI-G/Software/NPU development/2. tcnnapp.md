# 1. Introduction
This document describes how to use the sample/test applications provided by the TCC750x Linux Neural Network (NN) SDK. The SDK includes the tc-nn-app, a sample application that demonstrates how to execute neural network inference using the NPU (Neural Processing Unit) in various modes such as camera input, file input, and RTPM.

<br/><br/>

## 1.1 Overview

NPU Sample Application can only perform Neural Process Unit (NPU)
operations.

The tc-nn-app architecture as shown below.

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/1.%20Sample%20Application.png"></p>


<p align="center"><strong>Figure 1.1 Architecture of tc-nn-app</strong></p>

<br/><br/>

## 1.2 NPU INFERENCE FLOW

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/2.%20NPU%20Inference%20Flow.png"></p>
<p align="center"><strong>Figure 1.2 NPU Inference Flow</strong>


<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Step Description</strong></td>
<td><strong>Step Description</strong></td>
</tr>
<tr class="even">
<td>Open</td>
<td>Call npu_open to generate NPU Device Handle.</td>
</tr>
<tr class="odd">
<td>Update MLX Firmware</td>
<td>Load MLX Kernel firmware through npu_init_mlx and enable MLX.</td>
</tr>
<tr class="even">
<td>Create Network</td>
<td>Create a network handle using npu_load_network.</td>
</tr>
<tr class="odd">
<td>Create Input Buffer</td>
<td>Create an Input buffer through buffer_alloc.</td>
</tr>
<tr class="even">
<td>Create Output Buffer</td>
<td>Create an Onput buffer through buffer_alloc.</td>
</tr>
<tr class="odd">
<td>Read Input Image</td>
<td><p>The input buffer address is obtained through buffer_get_addr.</p>
<p>Load the image into the input buffer.</p>
<p>The original image data may require manipulation such as resize
according to the</p>
<p>neural network input image size and color format.</p></td>
</tr>
<tr class="even">
<td>Run Network Inference</td>
<td><p>Run network_issue_run to call the RUN_INFERENCE command to
HW.</p>
<p>Run network_wait_done to wait for the RUN_INFERENCE command to
end.</p></td>
</tr>
<tr class="odd">
<td>Read Output Tensor</td>
<td>Get output buffer address from buffer_get_addr</td>
</tr>
<tr class="even">
<td>Run Post Process</td>
<td>Post processing (if necessary)</td>
</tr>
</tbody>
</table>

<br/><br/>

## 1.3 Use case
This chapter describes use cases for running the tcnnapp application.

### 1) Multi-inference Using Camera Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application1.png"></p>
<p align="center"><strong>Figure Camera Input Operation Scenario</strong>

Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp
```

### 2) Multi-inference Using RTPM Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application2.png"></p>
<p align="center"><strong>Figure RTPM Input Operation Scenario</strong>

Example:
```
root@telechips-topst-ai-g-main:~# tccnnapp -i rtpm -o rtpm
```

### 3) Mult-inference Using File Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application3.png"></p>
<p align="center"><strong>Figure File Input Operation Scenario</strong>

Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -i file -p inputFilePath.png -o file -P outputFilePath.png
```

### 4) Multi-inference Using Camera Input and RTPM Output
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application4.png"></p>
<p align="center"><strong>Figure Camera Input and RTPM Output Operation Scenario</strong>

Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -o rtpm
```

### 5) Multi-inference Using File Input and Display Output
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application5.png"></p>
<p align="center"><strong>Figure File Input and Display Output Operation Scenario</strong>

Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -i file -p inputFilePath.png -o display

```

<br/><br/><br/>

## 1.4 NPU Sample Application Code

You can get a sample code to do the NPU test in the TOPST AI-G SDK.

- {TOPST_PATH}/build/tcc7500-main/tmp/work/cortexa53-telechips-linux/tc-nn-app/1.0.0-r0/git/

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td>
<p><strong>├── common</strong></p>
<p><strong>│   ├── camera</strong></p>
<p><strong>│   │   ├── camera_api.c</strong></p>
<p><strong>│   │   └── camera_api.h</strong></p>
<p><strong>│   ├── display</strong></p>
<p><strong>│   │   ├── display_api.c</strong></p>
<p><strong>│   │   ├── display_api.h</strong></p>
<p><strong>│   │   ├── scaler_api.c</strong></p>
<p><strong>│   │   └── scaler_api.h</strong></p>
<p><strong>│   ├── message</strong></p>
<p><strong>│   │   ├── message_api.c</strong></p>
<p><strong>│   │   └── message_api.h</strong></p>
<p><strong>│   ├── utils</strong></p>
<p><strong>│   │   ├── opencv_api.c</strong></p>
<p><strong>│   │   ├── opencv_api.h</strong></p>
<p><strong>│   │   ├── perf_api.c</strong></p>
<p><strong>│   │   ├── perf_api.h</strong></p>
<p><strong>│   │   ├── time_api.c</strong></p>
<p><strong>│   │   └── time_api.h</strong></p>
<p><strong>├── inlucde</strong></p>
<p><strong>│   ├── NnError.h</strong></p>
<p><strong>│   └── NnType.h</strong></p>
<p><strong>├── src</strong></p>
<p><strong>│   ├── NnAppMain.c</strong></p>
<p><strong>│   ├── NnAppMain.h</strong></p>
<p><strong>│   ├── NnDebug.c</strong></p>
<p><strong>│   ├── NnDebug.h</strong></p>
<p><strong>│   ├── NnMemory.c</strong></p>
<p><strong>│   ├── NnMemory.h</strong></p>
<p><strong>│   ├── NnNeuralNetwork.c</strong></p>
<p><strong>│   ├── NnNeuralNetwork.h</strong></p>
<p><strong>│   ├── NnPerf.c</strong></p>
<p><strong>│   ├── NnPerf.h</strong></p>
<p><strong>│   ├── NnProtocolManager.c</strong></p>
<p><strong>│   ├── NnRtpm.c</strong></p>
<p><strong>│   ├── NnRtpm.h</strong></p>
<p><strong>│   ├── NnSignalHandler.c</strong></p>
<p><strong>│   └── NnSignalHandler.h</strong></p>
</tr>
</tbody>
</table>

<br/>

## 1.4.1 Description of Source Files

This source file ("tc-nn-app") implements examples of using neural networks in single and dual modes for inference, processing input 
from a camera or a file. The “NnAppMain.c” file serves as the entry point of the program, enabling mode selection and output of 
results, while yolov5s_quantized and mobilenetv2_10_quantized define the neural network models. The camera, display, and 
scaler functionalities are modularized and provided as APIs for handling video input and output. Additionally, utility functions for 
message handling and drawing results are also included to support overall functionality. This structure is designed to easily use
various modes of the neural networks.

<br/>

### 1.4.2 Functions and Descriptions for Application
api 설명~


## 1.5 tc-nn-app Options

Options provided by **tc-nn-app** are shown in the table below.

<br/>

**Table 1.3 tc-nn-app Options:**

<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 44%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Option</strong></td>
<td><strong>Range</strong></td>
<td><strong>Default</strong></td>
<td><strong>Example</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">

<tr class="odd">
<td>Network1 Path</td>
<td>-n</td>
<td></td>
<td>/usr/share/yolo v3_tiny_quanti zed</td>
<td>-n /usr/share/yol ov3_tiny_quant ized</td>
<td>A mode for specifying the neural network of network 1.</td>
</tr>
<tr class="even">

<td>Network2 Path</td>
<td>-N</td>
<td></td>
<td>/usr/share/mo bilenetv2_7_qu antized/</td>
<td>-N /usr/share/ mobilenetv2_7_ quantized/</td>
<td>A mode for specifying the neural network of network 2.</td>
</tr>
<tr class="even">
<tr class="odd">
<td>Input Mode</td>
<td>-i</td>
<td>camera rtpm file</td>
<td>camera</td>
<td>
  -i camera
  <br/>
  -i rtpm
  <br/>
  -i file
</td>
<td><p>camera: A mode that takes input from the camera.</p>
<p>rtpm: A mode that takes input from the rtpm.</p>
<p>file: A mode that takes input from the image file.</p></td>
</tr>
<tr class="even">
<td>Output Mode</td>
<td>-o</td>
<td>display rtpm file</td>
<td>display</td>
<td>
  -o display
  <br/>
  -o rtpm
  <br/>
  -o file
</td>
<td><p>display: A mode that outputs to an LCD screen.</p>
<p>rtpm: A mode that outputs to rtpm.</p>
<p>file: A mode that outputs to an image file.</p></td>
</tr>
<tr class="odd">
<td>Input Width</td>
<td>-w</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-w 1920</td>
<td>A mode for specifying the width of the input.</td>
</tr>
<tr class="even">
<td>Input Height</td>
<td>-h</td>
<td>Up to 1080</td>
<td>1080</td>
<td>-h 1080</td>
<td>A mode for specifying the height of the input.</td>
</tr>
<tr class="odd">
<td>Output Width</td>
<td>-W</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-W 1920</td>
<td>A mode for specifying the width of the output.</td>
</tr>
<tr class="even">
<td>Output Height</td>
<td>-H</td>
<td>Up to 720</td>
<td>720</td>
<td>-H 720</td>
<td>A mode for specifying the height of the output.</td>
</tr>
<tr class="odd">
<td>Input Path</td>
<td>-p</td>
<td></td>
<td>/dev/video0</td>
<td>-p /dev/video0</td>
<td>A mode for specifying the input path.</td>
</tr>
<tr class="even">
<td>Output Path</td>
<td>-P</td>
<td></td>
<td>/dev/overlay</td>
<td>-P /dev/overlay</td>
<td>A mode for specifying the output path</td>
</tr>
<tr class="odd">
<td>Debug Mode</td>
<td>-g</td>
<td>off log file</td>
<td>off</td>
<td>
  -g off
  <br/>
  -g log
  <br/>
  -g file
</td>
<td><p>off: A mode that runs without debugging.</p>
<p>log: A mode that outputs debugging information as logs.</p>
<p>file: A mode that saves debugging information to a file.</p></td>
</tr>

<td>Output Position X</td>
<td>-X</td>
<td></td>
<td>0</td>
<td>-X 100</td>
<td>Set the starting x coordinate for the output image</td>
</tr>
<tr class="even">

<td>Output Position Y</td>
<td>-Y</td>
<td></td>
<td>0</td>
<td>-Y 100</td>
<td>Set the starting y coordinate for the output image</td>
</tr>
<tr class="even">

<td>NPU Debug Mode</td>
<td>-u off <br>-u log <br>-u file</td>
<td></td>
<td>Off</td>
<td>-u off <br>-u log <br>-u file</td>
<td>The default operation period: 1000 ms<br>
off: Run without NPU debugging<br>
log: Debug NPU performance as logs<br>
file: Debug NPU performance as a file<br>
</td>
</tr>
<tr class="even">

<td>NPU Running Mode</td>
<td>-a</td>
<td>async<br>sync</td>
<td>async</td>
<td>-a async<br>-a sync</td>
<td>Async: The NPU driver uses threads for inference.<br>
sync: Tasks are processed sequentially for inference, with profiling enabled.</td>
</tr>
<tr class="even">

</tbody>
</table>

<br/>
