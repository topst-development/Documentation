# 1. Introduction
This document describes how to use the sample/test applications provided by the TCC750x Linux Neural Network (NN) SDK. The SDK includes the tc-nn-app, a sample application that demonstrates how to execute neural network inference using the NPU (Neural Processing Unit) in various modes such as camera input, file input, and RTPM.

<br/><br/>

## 1.1 Overview

NPU Sample Application can only perform Neural Process Unit (NPU)
operations.

The tc-nn-app architecture as shown below.

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/1.%20Sample%20Applictation.png"></p>


<p align="center"><strong>Figure 1.1 Architecture of tc-nn-app</strong></p>

<br/><br/>

## 1.2 NPU INFERENCE FLOW

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/2.%20NPU%20Inference%20Flow.png"></p>
<p align="center"><strong>Figure 1.2 NPU Inference Flow</strong>


<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Step Description</strong></td>
<td><strong>Step Description</strong></td>
</tr>
<tr class="even">
<td>Open</td>
<td>Call npu_open to generate NPU Device Handle.</td>
</tr>
<tr class="odd">
<td>Update MLX Firmware</td>
<td>Load MLX Kernel firmware through npu_init_mlx and enable MLX.</td>
</tr>
<tr class="even">
<td>Create Network</td>
<td>Create a network handle using npu_load_network.</td>
</tr>
<tr class="odd">
<td>Create Input Buffer</td>
<td>Create an Input buffer through buffer_alloc.</td>
</tr>
<tr class="even">
<td>Create Output Buffer</td>
<td>Create an Onput buffer through buffer_alloc.</td>
</tr>
<tr class="odd">
<td>Read Input Image</td>
<td><p>The input buffer address is obtained through buffer_get_addr.</p>
<p>Load the image into the input buffer.</p>
<p>The original image data may require manipulation such as resize
according to the</p>
<p>neural network input image size and color format.</p></td>
</tr>
<tr class="even">
<td>Run Network Inference</td>
<td><p>Run network_issue_run to call the RUN_INFERENCE command to
HW.</p>
<p>Run network_wait_done to wait for the RUN_INFERENCE command to
end.</p></td>
</tr>
<tr class="odd">
<td>Read Output Tensor</td>
<td>Get output buffer address from buffer_get_addr</td>
</tr>
<tr class="even">
<td>Run Post Process</td>
<td>Post processing (if necessary)</td>
</tr>
</tbody>
</table>

<br/><br/>

## 1.3 NPU Sample Application Code

You can get a sample code to do the NPU test in the TOPST AI-G SDK.

- {TOPST_PATH}/build/tcc7500-main/tmp/work/cortexa53-telechips-linux/tc-nn-app/1.0.0-r0/git/

<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td>
<p><strong>├── common</strong></p>
<p><strong>│   ├── camera</strong></p>
<p><strong>│   │   ├── camera_api.c</strong></p>
<p><strong>│   │   └── camera_api.h</strong></p>
<p><strong>│   ├── display</strong></p>
<p><strong>│   │   ├── display_api.c</strong></p>
<p><strong>│   │   ├── display_api.h</strong></p>
<p><strong>│   │   ├── scaler_api.c</strong></p>
<p><strong>│   │   └── scaler_api.h</strong></p>
<p><strong>│   ├── message</strong></p>
<p><strong>│   │   ├── message_api.c</strong></p>
<p><strong>│   │   └── message_api.h</strong></p>
<p><strong>│   ├── utils</strong></p>
<p><strong>│   │   ├── opencv_api.c</strong></p>
<p><strong>│   │   ├── opencv_api.h</strong></p>
<p><strong>│   │   ├── perf_api.c</strong></p>
<p><strong>│   │   ├── perf_api.h</strong></p>
<p><strong>│   │   ├── time_api.c</strong></p>
<p><strong>│   │   └── time_api.h</strong></p>
<p><strong>├── inlucde</strong></p>
<p><strong>│   ├── NnError.h</strong></p>
<p><strong>│   └── NnType.h</strong></p>
<p><strong>├── src</strong></p>
<p><strong>│   ├── NnAppMain.c</strong></p>
<p><strong>│   ├── NnAppMain.h</strong></p>
<p><strong>│   ├── NnDebug.c</strong></p>
<p><strong>│   ├── NnDebug.h</strong></p>
<p><strong>│   ├── NnMemory.c</strong></p>
<p><strong>│   ├── NnMemory.h</strong></p>
<p><strong>│   ├── NnNeuralNetwork.c</strong></p>
<p><strong>│   ├── NnNeuralNetwork.h</strong></p>
<p><strong>│   ├── NnPerf.c</strong></p>
<p><strong>│   ├── NnPerf.h</strong></p>
<p><strong>│   ├── NnProtocolManager.c</strong></p>
<p><strong>│   ├── NnRtpm.c</strong></p>
<p><strong>│   ├── NnRtpm.h</strong></p>
<p><strong>│   ├── NnSignalHandler.c</strong></p>
<p><strong>│   └── NnSignalHandler.h</strong></p>
</tr>
</tbody>
</table>

<br/>

## 1.1 Description of Source Files

This source file ("tc-nn-app") implements examples of using neural networks in single and dual modes for inference, processing input 
from a camera or a file. The “NnAppMain.c” file serves as the entry point of the program, enabling mode selection and output of 
results, while yolov5s_quantized and mobilenetv2_10_quantized define the neural network models. The camera, display, and 
scaler functionalities are modularized and provided as APIs for handling video input and output. Additionally, utility functions for 
message handling and drawing results are also included to support overall functionality. This structure is designed to easily use
various modes of the neural networks.

<br/>

### 1.3.1 Functions and Descriptions for Application

Table 1.2 is the functions for the operation of the NPU Sample
Application and its explanation.

When creating an application using NPU, you can refer to the function in
Table 1.2.

<br/>

**Table 1.2 Functions and Descriptions:**

<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 27%" />
<col style="width: 53%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Function</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td rowspan="7">src/NnAppMain.c</td>
<td>NnparseArgs ()</td>
<td>Option Parsing</td>
</tr>
<tr class="odd">
<td><p>InputcaptureOpenDevice ()</p>
<p>InputcaptureSetConfig ()</p></td>
<td><p>Camera Device Configuration</p>
<p>(Device Open, Input Size, Input Format)</p></td>
</tr>
<tr class="even">
<td>VisualizeOpenDevice ()</td>
<td>Display Output use Overlay Device</td>
</tr>
<tr class="odd">
<td>InferenceInit ()</td>
<td>Network(NPU Device) Initialize</td>
</tr>
<tr class="even">
<td>InputcaptureGetBuffer ()</td>
<td>Get Camera Data</td>
</tr>
<tr class="odd">
<td>cvDrawFPS ()<br />
cvDrawMemInfio ()<br />
cvDrawNpuInfo1 ()<br />
cvDrawNpuInfo2 ()</td>
<td>Draw the results with OpenCV</td>
</tr>
<tr class="even">
<td>VisuallizeShow ()</td>
<td>Result Output with Display</td>
</tr>
<tr class="odd">
<td rowspan="2">src/NnInference.c</td>
<td>InferenceInit ()</td>
<td><p>Open NPU Device. And Load network file</p>
<p>(network CMD, network Param)</p></td>
</tr>
<tr class="even">
<td>InferenceRun ()</td>
<td><p>After executing inference, tensor data is obtained.</p>
<p>Tensor data is delivered to net.so,</p>
<p>and net.so performs Object detection</p></td>
</tr>
<tr class="odd">
<td rowspan="4">src/npu_api.c</td>
<td>npu_open ()</td>
<td>Open NPU device driver</td>
</tr>
<tr class="even">
<td>network_load ():</td>
<td><p>Pass the network cmd.bin and qunatized_network.bin files</p>
<p>as parameters, create a network instance</p>
<p>with network information,</p>
<p>and return the fd that can access it.</p></td>
</tr>
<tr class="odd">
<td>buffer_alloc ()</td>
<td>Creates CMA buffer for input/output</td>
</tr>
<tr class="even">
<td>network_profile ()</td>
<td>Inference is executed using the input/output buffer passed through
net_run_req. Return the cycle required until the NPU operation is
completed.</td>
</tr>
</tbody>
</table>

<br/>

### 1.3.2 Operation Flow Chart

Figure 1.3 is the process of using NPU in the NPU Sample Application.

You can refer to this flow when creating an application using NPU.

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application.png"></p>

<p align="center"><strong>Figure 1.3 NPU Operation Flow Charr</strong></p>

<br/><br/>

## 1.4 Run NPU Sample Application

<br/>

### 1.4.1 tc-nn-app Options

Options provided by **tc-nn-app** are shown in the table below.

<br/>

**Table 1.3 tc-nn-app Options:**

<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 44%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Option</strong></td>
<td><strong>Range</strong></td>
<td><strong>Default</strong></td>
<td><strong>Example</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">

<tr class="odd">
<td>Network1 Path</td>
<td>-n</td>
<td></td>
<td>/usr/share/yolo v3_tiny_quanti zed</td>
<td>-n /usr/share/yol ov3_tiny_quant ized</td>
<td>A mode for specifying the neural network of network 1.</td>
</tr>
<tr class="even">

<td>Network2 Path</td>
<td>-N</td>
<td></td>
<td>/usr/share/mo bilenetv2_7_qu antized/</td>
<td>-N /usr/share/ mobilenetv2_7_ quantized/</td>
<td>A mode for specifying the neural network of network 2.</td>
</tr>
<tr class="even">
<tr class="odd">
<td>Input Mode</td>
<td>-i</td>
<td>camera rtpm file</td>
<td>camera</td>
<td>
  -i camera
  <br/>
  -i rtpm
  <br/>
  -i file
</td>
<td><p>camera: A mode that takes input from the camera.</p>
<p>rtpm: A mode that takes input from the rtpm.</p>
<p>file: A mode that takes input from the image file.</p></td>
</tr>
<tr class="even">
<td>Output Mode</td>
<td>-o</td>
<td>display rtpm file</td>
<td>display</td>
<td>
  -o display
  <br/>
  -o rtpm
  <br/>
  -o file
</td>
<td><p>display: A mode that outputs to an LCD screen.</p>
<p>rtpm: A mode that outputs to rtpm.</p>
<p>file: A mode that outputs to an image file.</p></td>
</tr>
<tr class="odd">
<td>Input Width</td>
<td>-w</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-w 1920</td>
<td>A mode for specifying the width of the input.</td>
</tr>
<tr class="even">
<td>Input Height</td>
<td>-h</td>
<td>Up to 1080</td>
<td>1080</td>
<td>-h 1080</td>
<td>A mode for specifying the height of the input.</td>
</tr>
<tr class="odd">
<td>Output Width</td>
<td>-W</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-W 1920</td>
<td>A mode for specifying the width of the output.</td>
</tr>
<tr class="even">
<td>Output Height</td>
<td>-H</td>
<td>Up to 720</td>
<td>720</td>
<td>-H 720</td>
<td>A mode for specifying the height of the output.</td>
</tr>
<tr class="odd">
<td>Input Path</td>
<td>-p</td>
<td></td>
<td>/dev/video0</td>
<td>-p /dev/video0</td>
<td>A mode for specifying the input path.</td>
</tr>
<tr class="even">
<td>Output Path</td>
<td>-P</td>
<td></td>
<td>/dev/overlay</td>
<td>-P /dev/overlay</td>
<td>A mode for specifying the output path</td>
</tr>
<tr class="odd">
<td>Debug Mode</td>
<td>-g</td>
<td>off log file</td>
<td>off</td>
<td>
  -g off
  <br/>
  -g log
  <br/>
  -g file
</td>
<td><p>off: A mode that runs without debugging.</p>
<p>log: A mode that outputs debugging information as logs.</p>
<p>file: A mode that saves debugging information to a file.</p></td>
</tr>

<td>Output Position X</td>
<td>-X</td>
<td></td>
<td>0</td>
<td>-X 100</td>
<td>Set the starting x coordinate for the output image</td>
</tr>
<tr class="even">

<td>Output Position Y</td>
<td>-Y</td>
<td></td>
<td>0</td>
<td>-Y 100</td>
<td>Set the starting y coordinate for the output image</td>
</tr>
<tr class="even">

<td>NPU Debug Mode</td>
<td>-u off <br>-u log <br>-u file</td>
<td></td>
<td>Off</td>
<td>-u off <br>-u log <br>-u file</td>
<td>The default operation period: 1000 ms<br>
off: Run without NPU debugging<br>
log: Debug NPU performance as logs<br>
file: Debug NPU performance as a file<br>
</td>
</tr>
<tr class="even">

<td>NPU Running Mode</td>
<td>-a</td>
<td>async<br>sync</td>
<td>async</td>
<td>-a async<br>-a sync</td>
<td>Async: The NPU driver uses threads for inference.<br>
sync: Tasks are processed sequentially for inference, with profiling enabled.</td>
</tr>
<tr class="even">

</tbody>
</table>

<br/>

### 1.4.2 Run tc-nn-app

The example of running **tc-nn-app** is as follows.

<br/>

<table align="center">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">

    $ tcnnapp -p /dev/video0 -W 800 -H 480 -w 1920 -h 1080
    ----------------Parameter Info----------------
    [Network1]Network Path         : /usr/share/yolov5_quantized/
    [Network2]Network Path         : /usr/share/mobilenetv2_10_quantized/

    [Common]Input Mode             : camera
    [Common]Output Mode            : display
    [Common]Input Size             : 1920 x 1080
    [Common]Output Size            : 800 x 480
    [Common]Input Path             : /dev/video0
    [Common]Output Path            : /dev/overlay
    [Common]Output Position X      : 0
    [Common]Output Position Y      : 0

    [Debug]Debug Mode              : off
    [Debug]NPU Debug Mode          : off
    [Debug]NPU Running Mode        : AsyncMode
    ----------------Parameter End----------------
</tr>
</tbody>
</table>

<br/>

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/4.%20Sample%20Application.png"></p>

<p align="center"><strong>Figure 1.4 tc-nn-app Execution Screen</strong></p>
