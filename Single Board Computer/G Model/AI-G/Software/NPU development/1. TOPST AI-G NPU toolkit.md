
# 1. Introduction

The TOPST AI-G’s NPU is an HW neural network accelerator. It is optimized
for inference of vision application neural networks such as object
detection, image classification, and face detection.

The “TOPST-AI-G-Common-SW-NPU-Toolkit Guide” documentation describes how
to use the NPU Toolkit.

<br/><br/>

## 1.1 Overview

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/1.%20Overview%20npu%20toolkit.png"style="width:3.71875in;height:3.59514in" /></p>

<p align="center"><strong>Figure 1 TOPST AI-G SW Tool Kit Overview</strong></p>

- Converter
  * Converts a network file into internal network format (.bin)
  * Supports ONNX(PyTorch), TF-Lite, and CFG(Darknet)

<!-- -->

- Quantizer
  * Generates quantized network: float to int8

<!-- -->

- Compiler
  * Generates NPU Machine Codes using quantized network as input

<br/><br/>
# 2. TOPST AI-G NPU Tool Kit Download

- **Download link**: <[NPU Tool Kit](https://drive.google.com/file/d/1ZsL87sWxC8YQ06Ja1VYNbnWbDoLHyeAK/view?usp=drive_link)>
- The download is blocked, so please contact community.
**링크 맞는지 확인 필요**

<br/><br/>

## 2.1 TOPST AI-G NPU Tool Kit Structure

NPU development

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/2.%20tc-nn-toolkit%20Directory.png" width="550"></p>

<p align="center"><strong>Figure 2 TOPST AI-G NPU Tool Kit Directory</strong></p>

<br/>

**Table 2.1 tc-nn-toolkit:**

|                Directory                        |        Description          |
|-------------------------------------------------|-----------------------------|
| tc-nn-toolkit | You’re working directory    |
| EnlightSDK               | NPU SDK directory           |
| build_network                                   | Network lib Build directory |


<br/><br/>
TOPST AI-G NPU Toolkit was tested on a 64-bit PC with Ubuntu 20.04 and
Python 3.8. GPU is not required.



<br/><br/>

## 2.2 Package Installation

Before installing python packages, make sure that python3.8 and related
libraries were installed. The Python packages required by TOPST AI-G NPU
Toolkit are listed in requirements.txt files in the Toolkit package
directory.

```
$ sudo add-apt-repository ppa:deadsnakes/ppa -y
$ sudo apt install python3.8 python3.8-venv python3.8-dev python3.8-tk
$ python3.8 -m venv ./venv
$ source ./venv/bin/activate
$ pip install --upgrade pip 
Collecting pip
  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 20.9 MB/s
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.0.2
    Uninstalling pip-20.0.2:
      Successfully uninstalled pip-20.0.2
Successfully installed pip-23.3.2

$ pip install -r requirements.txt
Collecting certifi==2022.6.15 (from -r requirements.txt (line 1))
  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 14.2 MB/s eta 0:00:00
Collecting charset-normalizer==2.1.0 (from -r requirements.txt (line 2))
  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)
Collecting cycler==0.11.0 (from -r requirements.txt (line 3))
  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)
Collecting distro==1.7.0 (from -r requirements.txt (line 4))
  Downloading distro-1.7.0-py3-none-any.whl (20 kB)
Collecting flatbuffers==1.11 (from -r requirements.txt (line 5))
  Downloading flatbuffers-1.11-py2.py3-none-any.whl (15 kB)
Collecting fonttools==4.34.4 (from -r requirements.txt (line 6))
  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 18.6 MB/s eta 0:00:00
Collecting idna==3.3 (from -r requirements.txt (line 7))
Downloading idna-3.3-py3-none-any.whl (61 kB)
….

$ pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 --extra-index-url \https://download.pytorch.org/whl/cpu
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu
Collecting torch==1.12.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torch-1.12.0%2Bcpu-cp38-cp38-linux_x86_64.whl (189.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.0/189.0 MB 22.3 MB/s eta 0:00:00
Collecting torchvision==0.13.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.13.0%2Bcpu-cp38-cp38-linux_x86_64.whl (13.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 38.8 MB/s eta 0:00:00
$ pip install ./enlight_viewer/netron-3.5.4-py2.py3-none-any.whl
```

<br/><br/>

## 2.3 Sample Networks

- Sample Networks Path:
  input_networks/

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/3.%20toolkit_sample.png
" width="550"></p>
<p align="center"><strong>Figure 2.2 Sample Networks</strong></p>



  **Table 2.1 Sample Networks Description:**

|     Machine     |                        File                           |
|-----------------|-------------------------------------------------------|
| Darknet         | yolov4.cfg, yolov4.weights                            |
| TensorFlow Lite | lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite |
| ONNX            | mb2-ssd-lite.onnx, mb2-ssd-lite.anchor                |
| ONNX_YOLO       | yolov3/5/6/7/8/x.bin, yolov3/5/6/7/8/x_extracted.onnx | 


# 3. Build Guide

<br/><br/>

## 3.1 Converter

The Converter reads neural network file and converts it into ”bin”
file, which is network file format used in TOPST AI-G NPU SW Toolkit.
And it also performs network inferences and collects statistics of
activation data, which are used later for network quantization.

<br/>

<strong>1.  Darknet</strong>

```
$ python ./EnlightSDK/converter.py \
  ./input_networks/yolov4.cfg \
  --weight ./input_networks/yolov4.weights \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/yolov4.enlight \
  --enable-track \
  --mean 0 0 0 \
  --std 1 1 1 \
  --num-class 90
```

<br/>

<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/yolov4.cfg</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--weight ./input_networks/yolov4.weights</td>
<td>Darknet weights file name</td>
</tr>
<tr class="odd">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="even">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="odd">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="even">
<td>--output ./output_networks/yolov4.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="odd">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="even">
<td>--mean 0 0 0</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--std 1 1 1</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>

<br/>

<strong>2.  TensorFlow Lite</strong>

```
$ python ./EnlightSDK/converter.py \
  ./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --enable-track \
  --mean 0.5 0.5 0.5 \
  --std 0.5 0.5 0.5 \
  --num-class 90
```

<br/>

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output
./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.5 0.5 0.5</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.5 0.5 0.5</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>

<br/>

<strong>3.  ONNX</strong>

```
$ python ./EnlightSDK/converter.py
  ./input_networks/mb2-ssd-lite.onnx \
  --type obj \
  --add-detection-post-process ./input_networks/mb2-ssd-lite.anchor \
  --logistic softmax \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/mb2-ssd-lite.enlight \
  --enable-track \
  --mean 0.498 0.498 0.498 \
  --std 0.502 0.502 0.502 \
  --num-class 20 \
  --variance 0.1 0.2
```

<br/>

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/mb2-ssd-lite.onnx</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--add-detection-post-process
./input_networks/mb2-ssd-lite.anchor</td>
<td>Default box file name</td>
</tr>
<tr class="even">
<td>--logistic softmax</td>
<td><p>Logistic function in post process:</p>
<p>softmax, sigmoid, none</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output ./output_networks/mb2-ssd-lite.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.498 0.498 0.498</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.502 0.502 0.502</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 20</td>
<td>object class number</td>
</tr>
<tr class="even">
<td>--variance 0.1 0.2</td>
<td>Variance of x and y for SSD detection layer</td>
</tr>
</tbody>
</table>

<br/><br/>

## 3.2 Quantizer

The Quantizer converts the original network model based on floating
point operations into integer-based (quantized) network.

<br/>

**1.  Darknet**

```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/yolov4.enlight \
  --output ./output_networks/yolov4_quantized.enlight
```

<br/>

**2.  TensorFlow Lite**

```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight
```

<br/>

**3.  ONNX**

```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/mb2-ssd-lite.enlight \
  --output ./output_networks/mb2-ssd-lite_quantized.enlight
```

<br/><br/>

## 3.3 Compiler

The Compiler compiles a quantized neural network into NPU codes and
network parameters for processing neural networks on TOPST NPU HW.

<br/>

**1.  Darknet**

```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/yolov4_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

<br/>

**2.  TensorFlow Lite**

```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

<br/>

**3.  ONNX**

```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/mb2-ssd-lite_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

<br/>

<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<tbody>
<tr class="odd">
<td>--th-iou 0.5</td>
<td><p>IOU threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
<tr class="even">
<td>--th-conf 0.5</td>
<td><p>Confidence threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
</tbody>
</table>

<br/>

The generated files are stored in “output_code/\<network_name\>”
directory.

Compiler Output File:

- npu_cmd.bin: TELECHIPS NPU inference code
- quantized_network.bin: Network weight and bias parameters
- network.h: Buffers size and network parameters for post-process
- post_process.c: Post-process code
- Other files: Debugging information (internal only)

In neural network application, “npu_cmd.bin” and “quantized_network.bin”
are loaded into DMA buffers using TELECHIPS NPU APIs and Linux device
driver. “network.h” and “post_process.c” are built as network object
file (.so) as shown in Network Object Build.


<br/><br/>

## 3.4 Network Object Build

This section introduces how to build a network object file using a
compiled neural network file. The “build_network” directory contains
compilation environment to build shared library file (.so) of a network
model, which is used in TOPST AI Board.

<br/><br/>

### 3.4.1 Install Toolchain(gcc-arm-9.2)

```
$ wget https://developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ tar -xvf gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ echo "export PATH=~/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin:\$PATH" >> ~/.bashrc
$ source ~/.bashrc
$ aarch64-none-linux-gnu-gcc --version
aarch64-none-linux-gnu-gcc (GNU Toolchain for the A-profile Architecture 9.2-2019.12 (arm-9.10)) 9.2.1 20191025
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

<br/><br/>

### 3.4.2 Build Object

Copy compiler output directory to “build_network” directory.  
Before running make, you must delete the existing post_process.c and network.h in the build_nerwork folder.  
Then, network.h and post_process.c in the compiled <network_name>_quantized folder are imported into the build_network folder as symbolic links.


```
$ cd build_network
$ cp ../output_code/<network_name>_quantized/ ./ -ar
$ rm -rf network.h post_process.c
$ ln -s ./<network_name>_quantized/network.h
$ ln -s ./<network_name>_quantized/post_process.c
$ make
$ cp ./net.so ./<network_name>_quantized/
$ ls -l <network_name>_quantized/
compile_debug.log
net.so
network.h
network_buf.txt
network_group_info.csv
      ㆍ
      ㆍ
      ㆍ
```

# 4. Download Network

<br/>

1.  Booting TOPST AI-G Board.
2.  Transfer the Network folder using the “scp” command.


    ```
    $ scp -r yolov4_quantized/ root@192.168.0.100:/home/root
    The authenticity of host '192.168.0.100 (192.168.0.100)' can't be established.
    ED25519 key fingerprint is SHA256:UCT26gsSk4PfzL/lZ4c3o4RM/MSl16mQ49XSn7FRx/s.
    This key is not known by any other names
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    Warning: Permanently added '192.168.0.100' (ED25519) to the list of known hosts.	
    root@192.168.0.100's password: //<- enter root
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/4.%20scp%20Command.png"</p>
<p align="center"><strong>Figure 4 scp Command</strong></p>

3.  Check by below in the TOPST AI-G console.

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/5.%20Check%20Download.png" width="550"></p>

<p align="center"><strong>Figure 5 Check Download</strong></p>

4.  If enter by below command the TOPST AI-G console, run to sample
    application.

    ```
    $ tcnnapp -p /dev/video0 -W 800 -H 480 -w 1920 -h 1080 -n yolov4_quantized/
    ===================================================================================================
    ==   _________  _______   ___       _______   ________  ___  ___  ___  ________  ________        ==
    ==  |\___   ___\\  ___ \ |\  \     |\  ___ \ |\   ____\|\  \|\  \|\  \|\   __  \|\   ____\       ==
    ==  \|___ \  \_\ \   __/|\ \  \    \ \   __/|\ \  \___|\ \  \\\  \ \  \ \  \|\  \ \  \___|_      ==
    ==       \ \  \ \ \  \_|/_\ \  \    \ \  \_|/_\ \  \    \ \   __  \ \  \ \   ____\ \_____  \     ==
    ==        \ \  \ \ \  \_|\ \ \  \____\ \  \_|\ \ \  \____\ \  \ \  \ \  \ \  \___|\|____|\  \    ==
    ==         \ \__\ \ \_______\ \_______\ \_______\ \_______\ \__\ \__\ \__\ \__\     ____\_\  \   ==
    ==          \|__|  \|_______|\|_______|\|_______|\|_______|\|__|\|__|\|__|\|__|    |\_________\  ==
    ==                                                                                 \|_________|  ==
    ===================================================================================================

    ----------------Parameter Info----------------

    [Network1]Network Path         : yolov4_quantized/
    [Network2]Network Path         : /usr/share/mobilenetv2_10_quantized/

    [Common]Input Mode             : camera
    [Common]Output Mode            : display
    [Common]Input Size             : 1920 x 1080
    [Common]Output Size            : 800 x 480
    [Common]Input Path             : /dev/video0
    [Common]Output Path            : /dev/overlay
    [Common]Output Position X      : 0
    [Common]Output Position Y      : 0

    [Debug]Debug Mode              : off
    [Debug]NPU Debug Mode          : off
    [Debug]NPU Running Mode        : AsyncMode
    ----------------Parameter End----------------
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/6.%20Sample%20Applictation.png" width="500"></p>

<p align="center"><strong>Figure 6 Sample Application Run Screen</strong></p>




