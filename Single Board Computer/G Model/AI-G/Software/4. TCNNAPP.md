# 1. Introduction
---
This document describes how to use the sample/test applications provided by the AI-G SDK. The SDK includes the tcnnapp, a sample application that demonstrates how to execute neural network inference using the Neural Processing Unit (NPU) in various modes such as camera input, file input, and RTPM.
 
<br/><br/><br/>
 
## 1.1 Overview
---
NPU Sample Application can only perform Neural Process Unit (NPU)
operations.
 
The tcnnapp architecture as shown below.
 
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/1.%20Sample%20Application.png"></p>
 
 
<p align="center"><strong>Figure 1.1 Architecture of tcnnapp</strong></p>
 
<br/><br/><br/>
 
## 1.2 NPU Inference Flow
---
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/2.%20NPU%20Inference%20Flow.png"></p>
<p align="center"><strong>Figure 1.2 NPU Inference Flow</strong><br/>


**Table 1.1 NPU Inference Flow Description**
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Step</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>Open</td>
<td>Call npu_open to generate NPU device handle.</td>
</tr>
<tr class="odd">
<td>Update MLX Firmware</td>
<td>Load MLX Kernel firmware through npu_init_mlx and enable MLX.</td>
</tr>
<tr class="even">
<td>Create Network</td>
<td>Create a network handle using npu_load_network.</td>
</tr>
<tr class="odd">
<td>Create Input Buffer</td>
<td>Create an input buffer through buffer_alloc.</td>
</tr>
<tr class="even">
<td>Create Output Buffer</td>
<td>Create an output buffer through buffer_alloc.</td>
</tr>
<tr class="odd">
<td>Read Input Image</td>
<td><p>The input buffer address is obtained through buffer_get_addr.</p>
<p>Load the image into the input buffer.</p>
<p>The original image data may require manipulation such as resize
according to the</p>
<p>neural network input image size and color format.</p></td>
</tr>
<tr class="even">
<td>Run Network Inference</td>
<td><p>Run network_issue_run to call the RUN_INFERENCE command to
HW.</p>
<p>Run network_wait_done to wait for the RUN_INFERENCE command to
end.</p></td>
</tr>
<tr class="odd">
<td>Read Output Tensor</td>
<td>Get output buffer address from buffer_get_addr.</td>
</tr>
<tr class="even">
<td>Run Post Process</td>
<td>Post processing (if necessary).</td>
</tr>
</tbody>
</table>
 
<br/><br/><br/>
 
 
## 1.3 Use Cases
---
This chapter describes use cases for running the tcnnapp application.
<br/><br/>

### Multi-inference Using Camera Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application1.png"></p>
<p align="center"><strong>Figure 1.3 Camera Input Operation Scenario</strong><br/>
 
Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp
```
<br/><br/>
 
### Multi-inference Using RTPM Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application2.png"></p>
<p align="center"><strong>Figure 1.4 RTPM Input Operation Scenario</strong><br/>
 
Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -i rtpm -o rtpm
```
<br/><br/>
 
### Multi-inference Using File Input
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application3.png"></p>
<p align="center"><strong>Figure 1.5 File Input Operation Scenario</strong><br/>
 
Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -i file -p inputFilePath.png -o file -P outputFilePath.png
```
<br/><br/>
 
### Multi-inference Using Camera Input and RTPM Output
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application4.png"></p>
<p align="center"><strong>Figure 1.6 Camera Input and RTPM Output Operation Scenario</strong><br/>
 
Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -o rtpm
```
<br/><br/>
 
### Multi-inference Using File Input and Display Output
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/tcnnapp/3.%20Sample%20Application5.png"></p>
<p align="center"><strong>Figure 1.7 File Input and Display Output Operation Scenario</strong><br/>
 
Example:
```
root@telechips-topst-ai-g-main:~# tcnnapp -i file -p inputFilePath.png  display
 
```
<br/><br/><br/>
 
## 1.4 NPU Sample Application Code
--- 
You can get a sample code to do the NPU test in the AI-G SDK.
 
- {TOPST_PATH}/build/tcc7500-main/tmp/work/cortexa53-telechips-linux/tc-nn-app/1.0.0-r0/git/
 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="odd">
<td>
<p><strong>├── common</strong></p>
<p><strong>│   ├── camera</strong></p>
<p><strong>│   │   ├── camera_api.c</strong></p>
<p><strong>│   │   └── camera_api.h</strong></p>
<p><strong>│   ├── display</strong></p>
<p><strong>│   │   ├── display_api.c</strong></p>
<p><strong>│   │   ├── display_api.h</strong></p>
<p><strong>│   │   ├── scaler_api.c</strong></p>
<p><strong>│   │   └── scaler_api.h</strong></p>
<p><strong>│   ├── message</strong></p>
<p><strong>│   │   ├── message_api.c</strong></p>
<p><strong>│   │   └── message_api.h</strong></p>
<p><strong>│   ├── utils</strong></p>
<p><strong>│   │   ├── opencv_api.c</strong></p>
<p><strong>│   │   ├── opencv_api.h</strong></p>
<p><strong>│   │   ├── perf_api.c</strong></p>
<p><strong>│   │   ├── perf_api.h</strong></p>
<p><strong>│   │   ├── time_api.c</strong></p>
<p><strong>│   │   └── time_api.h</strong></p>
<p><strong>├── include</strong></p>
<p><strong>│   ├── NnError.h</strong></p>
<p><strong>│   └── NnType.h</strong></p>
<p><strong>├── src</strong></p>
<p><strong>│   ├── NnAppMain.c</strong></p>
<p><strong>│   ├── NnAppMain.h</strong></p>
<p><strong>│   ├── NnDebug.c</strong></p>
<p><strong>│   ├── NnDebug.h</strong></p>
<p><strong>│   ├── NnMemory.c</strong></p>
<p><strong>│   ├── NnMemory.h</strong></p>
<p><strong>│   ├── NnNeuralNetwork.c</strong></p>
<p><strong>│   ├── NnNeuralNetwork.h</strong></p>
<p><strong>│   ├── NnPerf.c</strong></p>
<p><strong>│   ├── NnPerf.h</strong></p>
<p><strong>│   ├── NnProtocolManager.c</strong></p>
<p><strong>│   ├── NnRtpm.c</strong></p>
<p><strong>│   ├── NnRtpm.h</strong></p>
<p><strong>│   ├── NnSignalHandler.c</strong></p>
<p><strong>│   └── NnSignalHandler.h</strong></p>
</tr>
</tbody>
</table>
 
<br/><br/><br/>
 
## 1.4.1 Description of Source Files
 
This source file ("tcnnapp") implements examples of using neural networks in single and dual modes for inference, processing input
from a camera or a file. The “NnAppMain.c” file serves as the entry point of the program, enabling mode selection and output of
results, while yolov5s_quantized and mobilenetv2_10_quantized define the neural network models. The camera, display, and
scaler functionalities are modularized and provided as APIs for handling video input and output. Additionally, utility functions for
message handling and drawing results are also included to support overall functionality. This structure is designed to easily use
various modes of the neural networks.
 
<br/><br/><br/>
 
 
## 1.4.2 Functions and Descriptions for Application
 
Table 1.1 is the functions for the operation of the NPU Sample
Application and its explanation.
 
When creating an application using NPU, you can refer to the function in
Table 1.1.
 
**Table 1.2 Functions and Descriptions:**
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 27%" />
<col style="width: 53%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Function</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td rowspan="12">APIs</td>
<td>npu_open()</td>
<td>This function acquires the system resources associated with the NPU cluster.</td>
</tr>
<tr class="odd">
<td><p>npu_close()</p></td>
<td><p>This function releases the system resources associated with the NPU cluster.</p></td>
</tr>
<tr class="even">
<td>npu_write_reg()</td>
<td>This function writes data to NPU device register.</td>
</tr>
<tr class="odd">
<td>npu_read_reg()</td>
<td>This function reads data from NPU device register.</td>
</tr>
<tr class="even">
<td>network_load()</td>
<td>This function loads network commands (cmd) and parameters (param) to the NPU DMA buffer.</td>
</tr>
<tr class="odd">
<td>network_run()</td>
<td>This function runs NPU to perform inference and waits until the inference is complete.</td>
</tr>
<tr class="even">
<td>network_get_input_size()</td>
<td>This function gets the buffer size of the network input image.</td>
</tr>
<tr class="even">
<td>network_get_output_size()</td>
<td>This function gets the buffer size of the network output image.</td>
</tr>
<tr class="even">
<td>network_get_input_width()</td>
<td>This function gets the width of the network input image.</td>
</tr>
<td>network_get_input_height()</td>
<td>This function gets the height of the network input image.</td>
</tr>
<tr class="even">
<td>network_get_type()</td>
<td>This function gets the type of network post-process.</td>
</tr>
<tr class="even">
<td>network_run_postprocess()</td>
<td>This function runs the network post-process.</td>
</tr>
 
<tr class="odd">
<td rowspan="8">Camera Interface</td>
<td>CameraCreate()</td>
<td><p>This function acquires the system resources associated with the camera handler.</p></td>
</tr>
<tr class="even">
<td>CameraDestroy()</td>
<td><p>This function releases the system resources associated with the camera handler.</p></td>
</tr>
<tr class="even">
<td>CameraOpenDevice()</td>
<td><p>This function allocates system resources to open the camera device.</p></td>
</tr>
<tr class="even">
<td>CameraCloseDevice()</td>
<td><p>This function releases camera buffer to close the camera device.</p></td>
</tr>
<tr class="even">
<td>CameraSetConfig()</td>
<td><p>This function sets up camera configuration.</p></td>
</tr>
<tr class="even">
<td>CameraGetBuffer()</td>
<td><p>This function gets the buffer of the camera input image.</p></td>
</tr>
<tr class="even">
<td>CameraReleaseBuffer()</td>
<td><p>This function releases the camera input buffer.</p></td>
</tr>
<tr class="even">
<td>CameraGetStatus()</td>
<td><p>This function gets the camera status.</p></td>
</tr>
<tr class="odd">
<td rowspan="5">Display Interface</td>
<td>DisplayCreate()</td>
<td>This function acquires the system resources associated with the display.</td>
</tr>
<tr class="even">
<td>DisplayDestroy ()</td>
<td><p>This function releases the system resources associated with the display.</p></td>
</tr>
<tr class="odd">
<td>DisplayOpenDevice ()</td>
<td>This function allocates system resources to open the display device.</td>
</tr>
<tr class="even">
<td>DisplayCloseDevice()</td>
<td>This function releases file descriptor (display_fd) to close the display device.</td>
</tr>
<tr class="even">
<td>DisplayShow()</td>
<td>This function displays the input image on the LCD.</td>
</tr>
<tr class="odd">
<td rowspan="8">Scaler Interface</td>
<td>ScalerCreate()</td>
<td>This function acquires the system resources associated with the scaler.</td>
</tr>
<tr class="even">
<td>ScalerDestroy()</td>
<td><p>This function releases the system resources associated with the scaler.</p></td>
</tr>
<tr class="odd">
<td>ScalerOpenDevice()</td>
<td>This function allocates system resources to open the scaler device.</td>
</tr>
<tr class="even">
<td>ScalerCloseDevice()</td>
<td>This function allocates system resources to close the scaler device.</td>
</tr>
<tr class="even">
<td>ScalerResize()</td>
<td>This function resizes the input image to create the final image.</td>
</tr>
<tr class="even">
<td>ScalerCropResize()</td>
<td>This function crops the input image to a specified region and resizes it to produce the final image.</td>
</tr>
<tr class="even">
<td>ScalerPoll()</td>
<td>This function configures the scaler's file descriptor and monitors for read events.</td>
</tr>
<tr class="even">
<td>ScalerGetStatus()</td>
<td>This function gets the scaler communication status.</td>
</tr>
<tr class="odd">
<td rowspan="9">Message Interface</td>
<td>MessageCreate()</td>
<td>This function acquires the system resources associated with the message.</td>
</tr>
<tr class="even">
<td>MessageDestroy()</td>
<td><p>This function releases the system resources associated with the message.</p></td>
</tr>
<tr class="odd">
<td>MessageOpen()</td>
<td>This function allocates system resources to open the message device.</td>
</tr>
<tr class="even">
<td>MessageClose()</td>
<td>This function allocates system resources to close the message device.</td>
</tr>
<tr class="even">
<td>MessagePopReceiveBuffer()</td>
<td>This function pops the receive buffer.</td>
</tr>
<tr class="even">
<td>MessagePushReceiveBuffer()</td>
<td>This function pushes the receive buffer.</td>
</tr>
<tr class="even">
<td>MessagePopSendBuffer()</td>
<td>This function pops the send buffer.</td>
</tr>
<tr class="even">
<td>MessagePushSendBuffer()</td>
<td>This function pushes the send buffer.</td>
</tr>
<tr class="even">
<td>MessageGetStatus()</td>
<td>This function gets the status of message communication.</td>
</tr>
<tr class="odd">
<td rowspan="8">OpenCV Interface</td>
<td>cvDrawInfo()</td>
<td>This function provides the text for drawing.</td>
</tr>
<tr class="even">
<td>cvDrawBoxes()</td>
<td><p>This function provides a bounding box for drawing.</p></td>
</tr>
<tr class="odd">
<td>cvDrawCls()</td>
<td>This function provides class information for drawing.</td>
</tr>
<tr class="even">
<td>cvLoadImage()</td>
<td>This function loads images.</td>
</tr>
<tr class="even">
<td>cvSaveImage()</td>
<td>This function saves images.</td>
</tr>
<tr class="even">
<td>cvResize()</td>
<td>This function resizes images.</td>
</tr>
<tr class="even">
<td>cvDrawPoints()</td>
<td>This function provides the point for drawing.</td>
</tr>
<tr class="even">
<td>cvDrawCustomText()</td>
<td>This function provides the user-defined text for drawing.</td>
</tr>
<tr class="odd">
<td rowspan="2">Performance Interface</td>
<td>getCpuUsage()</td>
<td>This function gets the information of the CPU usage.</td>
</tr>
<tr class="even">
<td>getMemoryUsage()</td>
<td><p>This function gets the information of the memory usage.</p></td>
</tr>
<tr class="odd">
<td rowspan="1">Time Interface</td>
 
<td>getCurrentTime()</td>
<td>This function gets the information of the current time.</td>
</tr>
 
 
</tbody>
</table>
 
 
<br/><br/><br/>
 
## 1.5 tcnnapp Options
--- 
Options provided by **tcnnapp** are shown in the table below.
 
 
**Table 1.2 tcnnapp Options:**
 
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 44%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Option</strong></td>
<td><strong>Range</strong></td>
<td><strong>Default</strong></td>
<td><strong>Example</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
 
<tr class="odd">
<td>Network1 Path</td>
<td>-n</td>
<td></td>
<td>/usr/share/yolo v3_tiny_quanti zed</td>
<td>-n /usr/share/yol ov3_tiny_quant ized</td>
<td>A mode for specifying the neural network of network 1.</td>
</tr>
<tr class="even">
 
<td>Network2 Path</td>
<td>-N</td>
<td></td>
<td>/usr/share/mo bilenetv2_7_qu antized/</td>
<td>-N /usr/share/ mobilenetv2_7_ quantized/</td>
<td>A mode for specifying the neural network of network 2.</td>
</tr>
<tr class="even">
<tr class="odd">
<td>Input Mode</td>
<td>-i</td>
<td>camera rtpm file</td>
<td>camera</td>
<td>
  -i camera
  <br/>
  -i rtpm
  <br/>
  -i file
</td>
<td><p>camera: A mode that takes input from the camera.</p>
<p>rtpm: A mode that takes input from the RTPM.</p>
<p>file: A mode that takes input from the image file.</p></td>
</tr>
<tr class="even">
<td>Output Mode</td>
<td>-o</td>
<td>display rtpm file</td>
<td>display</td>
<td>
  -o display
  <br/>
  -o rtpm
  <br/>
  -o file
</td>
<td><p>display: A mode that outputs to an LCD screen.</p>
<p>rtpm: A mode that outputs to the RTPM.</p>
<p>file: A mode that outputs to the image file.</p></td>
</tr>
<tr class="odd">
<td>Input Width</td>
<td>-w</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-w 1920</td>
<td>A mode for specifying the width of the input.</td>
</tr>
<tr class="even">
<td>Input Height</td>
<td>-h</td>
<td>Up to 1080</td>
<td>1080</td>
<td>-h 1080</td>
<td>A mode for specifying the height of the input.</td>
</tr>
<tr class="odd">
<td>Output Width</td>
<td>-W</td>
<td>Up to 1920</td>
<td>1920</td>
<td>-W 1920</td>
<td>A mode for specifying the width of the output.</td>
</tr>
<tr class="even">
<td>Output Height</td>
<td>-H</td>
<td>Up to 720</td>
<td>720</td>
<td>-H 720</td>
<td>A mode for specifying the height of the output.</td>
</tr>
<tr class="odd">
<td>Input Path</td>
<td>-p</td>
<td></td>
<td>/dev/video0</td>
<td>-p /dev/video0</td>
<td>A mode for specifying the input path.</td>
</tr>
<tr class="even">
<td>Output Path</td>
<td>-P</td>
<td></td>
<td>/dev/overlay</td>
<td>-P /dev/overlay</td>
<td>A mode for specifying the output path</td>
</tr>
<tr class="odd">
<td>Debug Mode</td>
<td>-g</td>
<td>off log file</td>
<td>off</td>
<td>
  -g off
  <br/>
  -g log
  <br/>
  -g file
</td>
<td><p>off: A mode that runs without debugging.</p>
<p>log: A mode that outputs debugging information as logs.</p>
<p>file: A mode that saves debugging information to a file.</p></td>
</tr>
 
<td>Output Position X</td>
<td>-X</td>
<td></td>
<td>0</td>
<td>-X 100</td>
<td>Set the starting x coordinate for the output image</td>
</tr>
<tr class="even">
 
<td>Output Position Y</td>
<td>-Y</td>
<td></td>
<td>0</td>
<td>-Y 100</td>
<td>Set the starting y coordinate for the output image</td>
</tr>
<tr class="even">
 
<td>NPU Debug Mode</td>
<td>-u off <br>-u log <br>-u file</td>
<td></td>
<td>Off</td>
<td>-u off <br>-u log <br>-u file</td>
<td>The default operation period: 1000 ms<br>
off: Run without NPU debugging<br>
log: Debug NPU performance as logs<br>
file: Debug NPU performance as a file<br>
</td>
</tr>
<tr class="even">
 
<td>NPU Running Mode</td>
<td>-a</td>
<td>async<br>sync</td>
<td>async</td>
<td>-a async<br>-a sync</td>
<td>Async: The NPU driver uses threads for inference.<br>
sync: Tasks are processed sequentially for inference, with profiling enabled.</td>
</tr>
<tr class="even">
 
</tbody>
</table>