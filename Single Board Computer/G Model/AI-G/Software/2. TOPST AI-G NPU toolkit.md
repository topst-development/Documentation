# 1. Introduction
---
Neural 
Processing Unit (NPU) is an HW neural network accelerator and the NPU in TCC750x is optimized for the inference of neural networks for vision applications, such as object detection, image classification, and face detection.  
This documentation describes how to use the NPU Toolkit.
<br/><br/><br/>

## 1.1 Overview
---
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/1.%20Overview%20npu%20toolkit.png"></p>
<p align="center"><strong>Figure 1.1 AI-G SW Tool Kit Overview</strong></p><br/>

- Converter
  * Converts a network file into internal network format (.bin)
  * Supports ONNX(PyTorch), TF-Lite, and CFG(Darknet)

<!-- -->

- Quantizer
  * Generates quantized network: float to int8

<!-- -->

- Compiler
  * Generates NPU machine codes by using the quantized network as input
<br/><br/><br/><br/>

# 2. AI-G NPU Toolkit Download
---
This chapter provides download information for the AI-G NPU Toolkit, which includes essential tools required for model compilation and execution on the platform.  
To download the NPU Toolkit, contact the TOPST community for access.
<br/><br/><br/>

## 2.1 AI-G NPU Toolkit Structure
---
NPU development
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/2.%20tc-nn-toolkit%20Directory.png" width="550"></p>
<p align="center"><strong>Figure 2.1 AI-G NPU Toolkit Directory</strong></p><br/>

**Table 2.1 tc-nn-toolkit:**

|                Directory                        |        Description          |
|-------------------------------------------------|-----------------------------|
| tc-nn-toolkit | Working directory    |
| EnlightSDK               | NPU SDK directory           |
| build_network                                   | Directory for building network libraries |

AI-G NPU Toolkit was tested on a 64-bit PC with Ubuntu 20.04 and
Python 3.8. GPU is not required.
<br/><br/><br/>

## 2.2 Package Installation
---
Before installing Python packages, ensure that Python3.8 and related
libraries are installed. The Python packages required by AI-G NPU Toolkit are listed in requirements.txt files in the Toolkit package
directory.
```
$ sudo add-apt-repository ppa:deadsnakes/ppa -y
$ sudo apt install python3.8 python3.8-venv python3.8-dev python3.8-tk
$ python3.8 -m venv ./venv
$ source ./venv/bin/activate
$ pip install --upgrade pip 
Collecting pip
  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 20.9 MB/s
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.0.2
    Uninstalling pip-20.0.2:
      Successfully uninstalled pip-20.0.2
Successfully installed pip-23.3.2

$ pip install -r requirements.txt
Collecting certifi==2022.6.15 (from -r requirements.txt (line 1))
  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 14.2 MB/s eta 0:00:00
Collecting charset-normalizer==2.1.0 (from -r requirements.txt (line 2))
  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)
Collecting cycler==0.11.0 (from -r requirements.txt (line 3))
  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)
Collecting distro==1.7.0 (from -r requirements.txt (line 4))
  Downloading distro-1.7.0-py3-none-any.whl (20 kB)
Collecting flatbuffers==1.11 (from -r requirements.txt (line 5))
  Downloading flatbuffers-1.11-py2.py3-none-any.whl (15 kB)
Collecting fonttools==4.34.4 (from -r requirements.txt (line 6))
  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 18.6 MB/s eta 0:00:00
Collecting idna==3.3 (from -r requirements.txt (line 7))
Downloading idna-3.3-py3-none-any.whl (61 kB)
….

$ pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 --extra-index-url \https://download.pytorch.org/whl/cpu
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu
Collecting torch==1.12.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torch-1.12.0%2Bcpu-cp38-cp38-linux_x86_64.whl (189.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.0/189.0 MB 22.3 MB/s eta 0:00:00
Collecting torchvision==0.13.0+cpu
  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.13.0%2Bcpu-cp38-cp38-linux_x86_64.whl (13.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 38.8 MB/s eta 0:00:00
$ pip install ./enlight_viewer/netron-3.5.4-py2.py3-none-any.whl
```
<br/><br/><br/>

## 2.3 Sample Networks
---
- Sample Networks Path:
  input_networks/

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/3.%20toolkit_sample.png" width="550"></p>
<p align="center"><strong>Figure 2.2 Sample Networks</strong></p><br/>

**Table 2.2 Sample Networks Description:**

|     Machine     |                        File                           |
|-----------------|-------------------------------------------------------|
| Darknet         | yolov4.cfg, yolov4.weights                            |
| TensorFlow Lite | lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite |
| ONNX            | mb2-ssd-lite.onnx, mb2-ssd-lite.anchor                |
| ONNX_YOLO       | yolov3/5/6/7/8/x.bin, yolov3/5/6/7/8/x_extracted.onnx | 
<br/><br/><br/><br/>

# 3. Build Guide
---
This chapter describes the process of converting and building neural network models for execution on the AI-G NPU. It begins with the use of the Converter tool, which transforms standard models into a format compatible with the NPU.
<br/><br/><br/>

## 3.1 Converter
---
The Converter reads neural network file and converts it into ”bin”
file, which is network file format used in AI-G NPU SW Toolkit.
It also performs network inferences and collects statistics of
activation data, which are used later for network quantization.

<strong>1.  Darknet</strong>
```
$ python ./EnlightSDK/converter.py \
  ./input_networks/yolov4.cfg \
  --weight ./input_networks/yolov4.weights \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/yolov4.enlight \
  --enable-track \
  --mean 0 0 0 \
  --std 1 1 1 \
  --num-class 90
```

<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/yolov4.cfg</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--weight ./input_networks/yolov4.weights</td>
<td>Darknet weights file name</td>
</tr>
<tr class="odd">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="even">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="odd">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="even">
<td>--output ./output_networks/yolov4.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="odd">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="even">
<td>--mean 0 0 0</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--std 1 1 1</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>
<br/><br/>

<strong>2.  TensorFlow Lite</strong>
```
$ python ./EnlightSDK/converter.py \
  ./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite \
  --type obj \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --enable-track \
  --mean 0.5 0.5 0.5 \
  --std 0.5 0.5 0.5 \
  --num-class 90
```

<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output
./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.5 0.5 0.5</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.5 0.5 0.5</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 90</td>
<td>object class number</td>
</tr>
</tbody>
</table>
<br/><br/>

<strong>3.  ONNX</strong>

The following process involves a two-step conversion using tc-nn-toolkit command-line interface:

**Step 1**: Convert the ONNX model with --type unknown, using --force-output to extract intermediate outputs before post-processing (e.g., box decode, scoring).

**Step 2**: Run a second conversion on the output from Step 1, applying post-processing options such as anchor boxes, logistic activation, and class settings to produce the final .enlight model.

This approach is required when the ONNX model includes unsupported operators (e.g., NonMaxSuppression).
Make sure that the layer names in --force-output match the actual output nodes in the ONNX graph.

```
$ python ./EnlightSDK/converter.py \
  ./input_networks/mb2-ssd-lite.onnx \
  --type obj \
  --add-detection-post-process ./input_networks/mb2-ssd-lite.anchor \
  --logistic softmax \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/mb2-ssd-lite.enlight \
  --enable-track \
  --mean 0.498 0.498 0.498 \
  --std 0.502 0.502 0.502 \
  --num-class 20 \
  --variance 0.1 0.2
```

### Step 1: Remove unsupported post-processing layers

```
python ./EnlightSDK/converter.py \
            ./input_networks/mb2-ssd-lite.onnx \
            --type unknown \
            --output ./output_networks/mb2-ssd-lite_wo_box_decode_score.enlight \
            --dataset-root my_dataset_path \
            --force-output Concat_0___Concat Concat_1___Concat_1
```

### Step 2: Apply post-processing

```
$ python ./EnlightSDK/converter.py \
  ./output_networks/mb2-ssd-lite_wo_box_decode_score.enlight \
  --type obj \
  --add-detection-post-process ./input_networks/mb2-ssd-lite.anchor \
  --logistic softmax \
  --dataset Custom \
  --dataset-root my_dataset_path \
  --output ./output_networks/mb2-ssd-lite.enlight \
  --enable-track \
  --mean 0.498 0.498 0.498 \
  --std 0.502 0.502 0.502 \
  --num-class 20 \
  --variance 0.1 0.2
```
<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td>./input_networks/mb2-ssd-lite_wo_box_decode_score.enlight</td>
<td>Input network file name</td>
</tr>
<tr class="even">
<td>--type obj</td>
<td><p>Type of post-processing:</p>
<p>classification (class),</p>
<p>object detection (obj),</p>
<p>or unknown (unknown)</p></td>
</tr>
<tr class="odd">
<td>--add-detection-post-process
./input_networks/mb2-ssd-lite.anchor</td>
<td>Default box file name</td>
</tr>
<tr class="even">
<td>--logistic softmax</td>
<td><p>Logistic function in post process:</p>
<p>softmax, sigmoid, none</p></td>
</tr>
<tr class="odd">
<td>--dataset Custom</td>
<td>Custom: user defined dataset</td>
</tr>
<tr class="even">
<td>--dataset-root ./my_dataset_path</td>
<td><p>Path to Input data which are used for</p>
<p>gathering activation statistics</p></td>
</tr>
<tr class="odd">
<td>--output ./output_networks/mb2-ssd-lite.enlight</td>
<td>Converter output file name</td>
</tr>
<tr class="even">
<td>--enable-track</td>
<td>Enable to gather activation statistics</td>
</tr>
<tr class="odd">
<td>--mean 0.498 0.498 0.498</td>
<td>mean values of input normalization (RGB)</td>
</tr>
<tr class="even">
<td>--std 0.502 0.502 0.502</td>
<td>std values of input normalization (RGB)</td>
</tr>
<tr class="odd">
<td>--num-class 20</td>
<td>object class number</td>
</tr>
<tr class="even">
<td>--variance 0.1 0.2</td>
<td>Variance of x and y for SSD detection layer</td>
</tr>
</tbody>
</table>
<br/><br/><br/>

## 3.2 Quantizer
---
The Quantizer converts the original network model based on floating
point operations into integer-based (quantized) network.

**1. Darknet**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/yolov4.enlight \
  --output ./output_networks/yolov4_quantized.enlight
```

**2. TensorFlow Lite**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.enlight \
  --output ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight
```

**3. ONNX**
```
$ python ./EnlightSDK/quantizer.py \
  ./output_networks/mb2-ssd-lite.enlight \
  --output ./output_networks/mb2-ssd-lite_quantized.enlight
```
<br/><br/><br/>

## 3.3 Compiler
---
The Compiler compiles a quantized neural network into NPU codes and
network parameters for processing neural networks on AI-G NPU HW.

**1.  Darknet**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/yolov4_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

**2.  TensorFlow Lite**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

**3.  ONNX**
```
$ python ./EnlightSDK/compiler.py \
  ./output_networks/mb2-ssd-lite_quantized.enlight \
  --th-iou 0.5 \
  --th-conf 0.5
```

<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<tbody>
<tr class="odd">
<td>--th-iou 0.5</td>
<td><p>IOU threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
<tr class="even">
<td>--th-conf 0.5</td>
<td><p>Confidence threshold.</p>
<p>Valid only for detection neural network.</p></td>
</tr>
</tbody>
</table>

The generated files are stored in the “output_code/\<network_name\>”
directory.

Compiler Output File:
- npu_cmd.bin: Inference code for Telechips NPU inference code
- quantized_network.bin: Network weight and bias parameters
- network.h: Buffer sizes and network parameters for post-process
- post_process.c: Post-process code
- Other files: Debugging information (internal only)

In neural network application, “npu_cmd.bin” and “quantized_network.bin”
are loaded into DMA buffers using Telechips NPU APIs and Linux device
driver. “network.h” and “post_process.c” are built as network object
file (.so) as shown in Network Object Build.
<br/><br/><br/>

## 3.4 Network Object Build
---
This chapter describes how to build a network object file using a
compiled neural network file. The “build_network” directory contains
compilation environment to build shared library file (.so) of a network
model, which is used in AI-G.
<br/><br/>

### 3.4.1 Install Toolchain(gcc-arm-9.2)
```
$ wget https://developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ tar -xvf gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
$ echo "export PATH=~/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin:\$PATH" >> ~/.bashrc
$ source ~/.bashrc
$ aarch64-none-linux-gnu-gcc --version
aarch64-none-linux-gnu-gcc (GNU Toolchain for the A-profile Architecture 9.2-2019.12 (arm-9.10)) 9.2.1 20191025
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```
<br/><br/>

### 3.4.2 Build Object
Copy the compiler output directory to the “build_network” directory.  
Before running the make command, you must delete the existing post_process.c and network.h files in the build_network folder.  
Then, create symbolic links to the network.h and post_process.c files from the compiled <network_name>_quantized directory into the build_network folder.

```
$ cd build_network
$ cp ../output_code/<network_name>_quantized/ ./ -ar
$ rm -rf network.h post_process.c
$ ln -s ./<network_name>_quantized/network.h
$ ln -s ./<network_name>_quantized/post_process.c
$ make
$ cp ./net.so ./<network_name>_quantized/
$ ls -l <network_name>_quantized/
compile_debug.log
net.so
network.h
network_buf.txt
network_group_info.csv
      ㆍ
      ㆍ
      ㆍ
```
<br/><br/><br/><br/>

# 4. Download Network
---
1. Connect the camera and DSI display and boot the AI-G.
<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/7.%20connect.png"</p>
<p align="center"><strong>Figure 4.1 connect cam and display</strong></p><br/>

2. Transfer the Network folder using the “scp” command.

    ```
    $ scp -r yolov4_quantized/ root@192.168.0.100:/home/root
    The authenticity of host '192.168.0.100 (192.168.0.100)' can't be established.
    ED25519 key fingerprint is SHA256:UCT26gsSk4PfzL/lZ4c3o4RM/MSl16mQ49XSn7FRx/s.
    This key is not known by any other names
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    Warning: Permanently added '192.168.0.100' (ED25519) to the list of known hosts.	
    root@192.168.0.100's password: //<- enter root
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/4.%20scp%20Command.png"</p>
<p align="center"><strong>Figure 4.2 scp Command</strong></p><br/>

3. Confirm that the folder transferred from the Host PC appears in the AI-G terminal.

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/5.%20Check%20Download.png" width="550"></p>

<p align="center"><strong>Figure 4.3 Check Download</strong></p><br/>

4. Enter the following command in the AI-G terminal to run the sample application.

    ```
    $ tcnnapp -p /dev/video0 -W 800 -H 480 -w 1920 -h 1080 -n yolov4_quantized/
    ===================================================================================================
    ==   _________  _______   ___       _______   ________  ___  ___  ___  ________  ________        ==
    ==  |\___   ___\\  ___ \ |\  \     |\  ___ \ |\   ____\|\  \|\  \|\  \|\   __  \|\   ____\       ==
    ==  \|___ \  \_\ \   __/|\ \  \    \ \   __/|\ \  \___|\ \  \\\  \ \  \ \  \|\  \ \  \___|_      ==
    ==       \ \  \ \ \  \_|/_\ \  \    \ \  \_|/_\ \  \    \ \   __  \ \  \ \   ____\ \_____  \     ==
    ==        \ \  \ \ \  \_|\ \ \  \____\ \  \_|\ \ \  \____\ \  \ \  \ \  \ \  \___|\|____|\  \    ==
    ==         \ \__\ \ \_______\ \_______\ \_______\ \_______\ \__\ \__\ \__\ \__\     ____\_\  \   ==
    ==          \|__|  \|_______|\|_______|\|_______|\|_______|\|__|\|__|\|__|\|__|    |\_________\  ==
    ==                                                                                 \|_________|  ==
    ===================================================================================================

    ----------------Parameter Info----------------

    [Network1]Network Path         : yolov4_quantized/
    [Network2]Network Path         : /usr/share/mobilenetv2_10_quantized/

    [Common]Input Mode             : camera
    [Common]Output Mode            : display
    [Common]Input Size             : 1920 x 1080
    [Common]Output Size            : 800 x 480
    [Common]Input Path             : /dev/video0
    [Common]Output Path            : /dev/overlay
    [Common]Output Position X      : 0
    [Common]Output Position Y      : 0

    [Debug]Debug Mode              : off
    [Debug]NPU Debug Mode          : off
    [Debug]NPU Running Mode        : AsyncMode
    ----------------Parameter End----------------
    ```

<p align="center"><img src="https://raw.githubusercontent.com/topst-development/Documentation/refs/heads/main/Assets/TOPST%20AI-G/Software/NPU%20development/AI-G%20NPU%20toolkit/6.%20Sample%20Applictation.png" width="500"></p>
<p align="center"><strong>Figure 4.4 Sample Application Run Screen</strong></p><br/><br/><br/><br/>

# 5. NPU Compatible Operators
---
This chapter introduces the operators supported by the NPU. The following list includes the key operations that are compatible with the platform.
<br/><br/><br/>

## 5.1 AI-G NPU Supported Layer Type
---
The AI-G NPU supports the following layer types.

**Table 5.1 Supported Layer Types and Specifications:**

|        Layer Type       |                        Specification                        |
|-------------------------|-------------------------------------------------------------|
| Convolution             | Kernel size: 1x1 to 7x7<br/>Stride: 1 to 4<br/>Padding: 0 to 3 <br/>Dilation: 1 to 4 |
| ConvTranspose           | Kernel size: 2x2 to 3x3<br/>Stride: 2<br/>Padding: 0 to 1  |
| Depth-wise Convolution  | Kernel size: 1x1 to 5x5<br/>Stride: 1 to 2<br/>Padding: 0 to 2|
| Activation Function     | Bypass<br/>ReLU<br/>ReLU6<br/>Leaky Relu <br/> Sigmoid <br/> Tanh <br/> Mish <br/> Swish <br/> Clip <br/> HardSwish <br/> HardSigmoid| 
| Max Pool 2D             | Kernel size: 2x2 to 13x13<br/>Stride: 1 to 2<br/>Padding: 0 to 6| 
| Average Pool 2D         | Kernel size: 2x2 to 19x19<br/>Stride: 1 to 2<br/>Padding: 0 to 6| 
| Global Average Pool     | Supported                                                   | 
| Element-wise Operation  | Addition: Scalar + Tensor, Vector + Tensor, Tensor + Tensor <br/>  Multiplication: Scalar * Tensor, Vector * Tensor, Tensor * Tensor                    |  
| Batch Normalization     | Supported                                                   |  
| Concatenation           | Channel-wise only                                           | 
| Slice                   | Supported                                                   |
| Split                   | Supported                                                   |
<br/><br/><br/>

## 5.2 PyTorch Operators (ONNX)
---
Even if the operator is not supported by the AI-G NPU, it can still be executed by the Host CPU if the operation is in the final layer of the neural network.

**Note**: Softmax layers of Single Shot MultiBox Detector (SSD) networks are processed by the CPU in the post-processing codes.


|         Operator        |                         Dedicated HW                        |
|-------------------------|-------------------------------------------------------------|
| Conv                    | Kernel: 1x1 to 7x7<br/>Padding: 0 to 3<br/>Stride: 1 to 4<br/>Dilation: 1 to 4  | 
| Conv Transpose          |  Kernel: 2x2 to 3x3<br/>Padding: 0 to 1<br/>Stride: 2<br/>Output_padding: 0 to 1  |                                              |  
| Dw-conv                 | Kernel: 1x1 to 5x5<br/>Padding: 0 to 2<br/>Stride: 1 to 2<br/>Dilation: 1  |
| Activation              | Relu<br/> LeakyRelu<br/> Sigmoid<br/> Tanh<br/> Mish<br/> Clip (min = 0, only)<br/> HardSwish<br/> HardSigmoid |
| MaxPool                 | **Dedicated HW** <br/>Kernel: 2x2 only <br/> Stride: 2 only<br/>**Vector Processor** <br/>Kernel: 2x2 to 13x13<br/>Padding: 0 to 6<br/>Stride: 1 to 2   |
| AveragePool             | Kernel: 2x2 to 19x19<br/>Padding: 0 to 6<br/>Stride: 1 to 2 |
| GlobalAveragePool       | O                                                           |
| Add                     | **Dedicated HW**<br/>Scalar + Tensor <br/> Tensor + Tensor <br/>**Vector Processor** <br/>Vector + Tensor |
| Sub                     | O (Converted to Add operator)                                 |
| Mul                     | **Dedicated HW** <br/> Scalar * Tensor <br/> Tensor * Tensor<br/>  **Vector Processor** <br/> Tensor * Tensor                                                   |
| Div                     | O (Constant divisor only)                                   |
| Resize                  | Nearest neighborhood: 2x, 4x, 8x (=2^n) <br/>Bilinear: 2x, 4x, (align_core = false, only) |
| Upsample                | Stride: 2 only                                              |
| Batchnorm               | O                                                           |
| Pad                     | Padding: 0 to 3 (zero-padding only)                         |
| Concat                  | Direction: channel-wise only                                |
| Slice                   | O                                                           |
| Split                   | O                                                           |
| Squeeze                 | O                                                           |
| Unsqueeze               | O                                                           |
| MatMul                  | One tensor should be constant.                              |
| Gemm                    | O                                                           |
| Flatten                 | O                                                           |
| ReduceMean              | axes: [h,w] only                                            |
| ReduceSum               | axes: [h,w] only                                            |
| Constant                | O                                                           |
| ConstantOfShape         | O                                                           |
<br/><br/><br/>

## 5.3 Darknet Operators
---
Even if the operator is not supported by the AI-G NPU, it can still be executed by the Host CPU if the operation is in the final layer of the neural network.

|         Operator        |                         Dedicated HW                        |
|-------------------------|-------------------------------------------------------------|
| Conv2d                  | Kernel: 1x1 to 7x7<br/>Padding: 0 to 3<br/>Stride: 1 to 4<br/>Dilation: 1 to 4 |
| Deconvolution           | Kernel: 2x2 to 3x3<br/>Padding: 0 to 1<br/>Stride: 2        |
| Dw-conv                 | Kernel: 1x1 to 5x5<br/>Padding: 0 to 2<br/>Stride: 1 to 2<br/>Dilation: 1|
| Activation              | Relu<br/>Relu6<br/>Leaky<br/>Logistic<br/>Tanh<br/>Mish<br/>Swish|
| MaxPool                 | **Dedicated HW**<br/> Kernel: 2x2 only<br/>Stride: 2 only<br/>**Vector Processor**<br/> Kernel: 2x2 to 13x13<br/>Padding: 0 to 6 <br/>Stride: 1to 2  |
| Local_avgpool           | Kernel: 2x2 to 19x19<br/>Padding: 0 to 6<br/>Stride: 1 to 2 |
| Avgpool                 | O                                                           |
| Upsample                | Stride: 2 only                                              |
| Batchnorm               | O                                                           |
| Route                   | Direction: channel-wise only (32-aligned channel size only) |
| Connected               | O                                                           |
| Reorg3d                 | O                                                           |
| Shortcut                | O                                                           |
| Sam                     | O                                                           |
| Region                  | **Host CPU** <br/>O                                         |
| Yolo                    | **Host CPU** <br/>O                                         |
<br/><br/><br/>

## 5.4 Tensor Flow Lite Operators
---
Even if the operator is not supported by the AI-G NPU, it can still be executed by the Host CPU if the operation is in the final layer of the neural network.

|         Operator        |                         Dedicated HW                        |
|-------------------------|-------------------------------------------------------------|
| CONV_2D                 | Kernel: 1x1 to 7x7<br/>Padding: 0 to 3<br/>Stride: 1 to 4<br/>Dilation: 1 to 4 |
| TRANSPOSE_CONV          | Kernel: 2x2 to 3x3<br/>Padding: Same, Valid<br/>Stride: 2   |
| DEPTHWISE_CONV_2D       | Kernel: 1x1 to 5x5<br/>Padding: 0 to 2<br/>Stride: 1 to 2<br/>Dilation: 1|
| Activation              | RELU<br/>RELU6<br/>LEAKY_RELU<br/>LOGISTIC<br/>HARD_SWISH   |
| MAX_POOL_2D             | **Dedicated HW**<br/>Kernel: 2x2 only<br/>Stride: 2 only <br/>**Vector Processor** <br/> Kernel: 2x2 to 13x13<br/> Padding: 0 to 6<br/>Stride: 1 to 2|
| AVERAGE_POOL_2D         | Kernel: 2x2 to 19x19<br/> Padding: 0 to 6<br/>Stride: 1 to 2|  
| ADD                     | **Dedicated HW**<br/>Scalar + Tensor<br/>Tensor + Tensor<br/>**Vector Processor**<br/>Vector + Tensor     | 
| SUB                     | O (Converted to ADD layer)                                  | 
| MUL                     | **Dedicated HW**<br/>Scalar * Tensor<br/>Tensor * Tensor<br/>**Vector Processor**<br/>Tensor * Tensor     | 
| DIV                     | O(Constant divisor only)                                 |
| RESIZE                  | Nearest neighborhood: 2x, 4x, 8x (=2^n)<br/>Bilinear: 2x, 4x|
| PAD                     | Padding: 0 to 3 (zero-padding only)                         |
| CONCATENATION           | Direction: channel-wise only                                | 
| FULLY_CONNECTED         | O                                                           |
| MEAN                    | axes: [h,w] only                                            |   

